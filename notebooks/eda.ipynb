{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c357791",
   "metadata": {},
   "outputs": [],
   "source": "# An√°lise Explorat√≥ria de Dados (EDA) - Detec√ß√£o de Ataques de Rede\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura√ß√£o de visualiza√ß√£o\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\nprint(\"üìä Iniciando An√°lise Explorat√≥ria de Dados\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61d8c5",
   "metadata": {},
   "outputs": [],
   "source": "# 1. CARREGAMENTO E VIS√ÉO GERAL DOS DADOS\n\n# Carregamento dos dados processados\ndf = pd.read_csv('../data/processed/flows.csv')\n\nprint(f\"‚úÖ Dados carregados com sucesso!\")\nprint(f\"üìà Dimens√µes do dataset: {df.shape[0]} fluxos, {df.shape[1]} features\")\nprint(f\"üîç Primeiras linhas:\")\ndisplay(df.head())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b765b",
   "metadata": {},
   "outputs": [],
   "source": "# 2. ESTRUTURA E TIPOS DE DADOS\n\nprint(\"üî¨ Informa√ß√µes sobre os dados:\")\nprint(f\"üìä Formato: {df.shape}\")\nprint(f\"üíæ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\nprint()\n\nprint(\"üìã Tipos de dados:\")\ndisplay(df.dtypes.to_frame('Tipo'))\nprint()\n\nprint(\"‚ùå Valores ausentes:\")\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Valores Ausentes': missing,\n    'Percentual (%)': missing_pct\n}).round(2)\ndisplay(missing_df[missing_df['Valores Ausentes'] > 0])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1022df",
   "metadata": {},
   "outputs": [],
   "source": "# 3. ESTAT√çSTICAS DESCRITIVAS\n\nprint(\"üìà Resumo estat√≠stico das features num√©ricas:\")\nnumeric_cols = ['bytes', 'pkts', 'duration', 'iat_mean', 'iat_std']\ndesc_stats = df[numeric_cols + ['label']].describe()\ndisplay(desc_stats.round(4))\n\nprint()\nprint(\"üéØ Estat√≠sticas por classe (Normal vs Ataque):\")\nstats_by_label = df.groupby('label')[numeric_cols].describe()\ndisplay(stats_by_label.round(4))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d7ba5",
   "metadata": {},
   "outputs": [],
   "source": "# 4. AN√ÅLISE DE BALANCEAMENTO DAS CLASSES\n\nprint(\"‚öñÔ∏è Distribui√ß√£o das classes:\")\nlabel_counts = df['label'].value_counts().sort_index()\nlabel_props = df['label'].value_counts(normalize=True).sort_index()\n\nbalance_df = pd.DataFrame({\n    'Classe': ['Normal (0)', 'Ataque (1)'],\n    'Quantidade': label_counts.values,\n    'Propor√ß√£o (%)': (label_props.values * 100).round(2)\n})\n\ndisplay(balance_df)\n\n# Visualiza√ß√£o\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Gr√°fico de barras\nbalance_df.plot(x='Classe', y='Quantidade', kind='bar', ax=ax1, color=['skyblue', 'salmon'])\nax1.set_title('üìä Distribui√ß√£o Absoluta das Classes')\nax1.set_xlabel('Classe')\nax1.set_ylabel('N√∫mero de Fluxos')\nax1.tick_params(axis='x', rotation=0)\n\n# Gr√°fico de pizza\nax2.pie(balance_df['Quantidade'], labels=balance_df['Classe'], autopct='%1.1f%%', \n        colors=['skyblue', 'salmon'], startangle=90)\nax2.set_title('ü•ß Propor√ß√£o das Classes')\n\nplt.tight_layout()\nplt.show()\n\n# An√°lise de balanceamento\nratio = label_counts.iloc[1] / label_counts.iloc[0]\nprint(f\"üìä Propor√ß√£o Ataque/Normal: {ratio:.3f}\")\nif ratio < 0.1:\n    print(\"‚ö†Ô∏è  Dataset muito desbalanceado - considere t√©cnicas de balanceamento\")\nelif ratio < 0.5:\n    print(\"‚ö†Ô∏è  Dataset moderadamente desbalanceado\")\nelse:\n    print(\"‚úÖ Dataset relativamente balanceado\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c0626",
   "metadata": {},
   "outputs": [],
   "source": "# 5. DISTRIBUI√á√ïES UNIVARIADAS DAS FEATURES\n\nprint(\"üìä Analisando distribui√ß√µes das features num√©ricas...\")\n\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    # Histograma com curva de densidade\n    axes[i].hist(df[col], bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n    \n    # Estat√≠sticas da distribui√ß√£o\n    mean_val = df[col].mean()\n    median_val = df[col].median()\n    std_val = df[col].std()\n    \n    # Linhas de refer√™ncia\n    axes[i].axvline(mean_val, color='red', linestyle='--', label=f'M√©dia: {mean_val:.2f}')\n    axes[i].axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')\n    \n    axes[i].set_title(f'üìà Distribui√ß√£o de {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Densidade')\n    axes[i].legend()\n    axes[i].grid(True, alpha=0.3)\n    \n    # Teste de normalidade\n    _, p_value = stats.normaltest(df[col].dropna())\n    normality = \"Normal\" if p_value > 0.05 else \"N√£o Normal\"\n    axes[i].text(0.02, 0.98, f'Normalidade: {normality}\\nSkew: {df[col].skew():.2f}', \n                transform=axes[i].transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\n# Remove o subplot extra\nif len(numeric_cols) < len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüîç Interpreta√ß√£o das distribui√ß√µes:\")\nfor col in numeric_cols:\n    skew = df[col].skew()\n    print(f\"‚Ä¢ {col}: Assimetria = {skew:.2f} \", end=\"\")\n    if abs(skew) < 0.5:\n        print(\"(distribui√ß√£o aproximadamente sim√©trica)\")\n    elif abs(skew) < 1:\n        print(\"(distribui√ß√£o moderadamente assim√©trica)\")\n    else:\n        print(\"(distribui√ß√£o altamente assim√©trica)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48712b",
   "metadata": {},
   "outputs": [],
   "source": "# 6. AN√ÅLISE COMPARATIVA POR CLASSE (NORMAL VS ATAQUE)\n\nprint(\"üéØ Comparando distribui√ß√µes entre classes Normal e Ataque...\")\n\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    # Boxplot comparativo\n    box_data = [df[df['label'] == 0][col].dropna(), df[df['label'] == 1][col].dropna()]\n    box = axes[i].boxplot(box_data, labels=['Normal', 'Ataque'], patch_artist=True)\n    \n    # Colorir as caixas\n    colors = ['lightblue', 'lightcoral']\n    for patch, color in zip(box['boxes'], colors):\n        patch.set_facecolor(color)\n    \n    axes[i].set_title(f'üì¶ {col} por Classe')\n    axes[i].set_ylabel(col)\n    axes[i].grid(True, alpha=0.3)\n    \n    # Estat√≠sticas comparativas\n    normal_mean = df[df['label'] == 0][col].mean()\n    attack_mean = df[df['label'] == 1][col].mean()\n    \n    # Teste t para diferen√ßa de m√©dias\n    _, p_value = stats.ttest_ind(\n        df[df['label'] == 0][col].dropna(), \n        df[df['label'] == 1][col].dropna()\n    )\n    \n    significance = \"Significativa\" if p_value < 0.05 else \"N√£o Significativa\"\n    \n    axes[i].text(0.02, 0.98, \n                f'Normal: {normal_mean:.2f}\\nAtaque: {attack_mean:.2f}\\nDif: {significance}', \n                transform=axes[i].transAxes, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n\n# Remove o subplot extra\nif len(numeric_cols) < len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# An√°lise quantitativa das diferen√ßas\nprint(\"\\nüìä Resumo das diferen√ßas entre classes:\")\ncomparison_stats = []\n\nfor col in numeric_cols:\n    normal_data = df[df['label'] == 0][col]\n    attack_data = df[df['label'] == 1][col]\n    \n    _, p_value = stats.ttest_ind(normal_data.dropna(), attack_data.dropna())\n    effect_size = (attack_data.mean() - normal_data.mean()) / df[col].std()\n    \n    comparison_stats.append({\n        'Feature': col,\n        'Normal_Mean': normal_data.mean(),\n        'Attack_Mean': attack_data.mean(),\n        'Diferen√ßa_%': ((attack_data.mean() - normal_data.mean()) / normal_data.mean() * 100),\n        'P_Value': p_value,\n        'Effect_Size': effect_size\n    })\n\ncomparison_df = pd.DataFrame(comparison_stats)\ndisplay(comparison_df.round(4))\n\nprint(\"\\nüîç Features mais discriminativas (por tamanho do efeito):\")\ntop_features = comparison_df.reindex(comparison_df['Effect_Size'].abs().sort_values(ascending=False).index)\nfor _, row in top_features.head(3).iterrows():\n    print(f\"‚Ä¢ {row['Feature']}: Effect Size = {row['Effect_Size']:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01905fd",
   "metadata": {},
   "outputs": [],
   "source": "# 7. AN√ÅLISE DE CORRELA√á√ïES\n\nprint(\"üîó Analisando correla√ß√µes entre features...\")\n\n# Matriz de correla√ß√£o\ncorr_matrix = df[numeric_cols].corr()\n\n# Visualiza√ß√£o da matriz de correla√ß√£o\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Heatmap principal\nsns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=ax1)\nax1.set_title('üå°Ô∏è Matriz de Correla√ß√£o (Pearson)')\n\n# Heatmap apenas das correla√ß√µes fortes (|r| > 0.5)\nstrong_corr = corr_matrix.copy()\nstrong_corr[abs(strong_corr) < 0.5] = 0\nsns.heatmap(strong_corr, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=ax2)\nax2.set_title('üî• Correla√ß√µes Fortes (|r| ‚â• 0.5)')\n\nplt.tight_layout()\nplt.show()\n\n# Identificar correla√ß√µes mais fortes\nprint(\"\\nüìà Correla√ß√µes mais fortes entre features:\")\n# Criar matriz triangular superior para evitar duplicatas\nupper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\ncorrelation_pairs = []\n\nfor i in range(len(corr_matrix.columns)):\n    for j in range(i+1, len(corr_matrix.columns)):\n        if upper_triangle[i, j]:\n            feature1 = corr_matrix.columns[i]\n            feature2 = corr_matrix.columns[j]\n            correlation = corr_matrix.iloc[i, j]\n            correlation_pairs.append({\n                'Feature_1': feature1,\n                'Feature_2': feature2,\n                'Correla√ß√£o': correlation,\n                'Magnitude': abs(correlation)\n            })\n\ncorrelation_df = pd.DataFrame(correlation_pairs)\ncorrelation_df = correlation_df.sort_values('Magnitude', ascending=False)\n\nprint(\"Top 5 correla√ß√µes mais fortes:\")\ndisplay(correlation_df.head().round(3))\n\n# Interpreta√ß√£o das correla√ß√µes\nprint(\"\\nüîç Interpreta√ß√£o das correla√ß√µes:\")\nfor _, row in correlation_df.head(3).iterrows():\n    corr_val = row['Correla√ß√£o']\n    if abs(corr_val) >= 0.7:\n        strength = \"muito forte\"\n    elif abs(corr_val) >= 0.5:\n        strength = \"forte\"\n    elif abs(corr_val) >= 0.3:\n        strength = \"moderada\"\n    else:\n        strength = \"fraca\"\n    \n    direction = \"positiva\" if corr_val > 0 else \"negativa\"\n    print(f\"‚Ä¢ {row['Feature_1']} ‚Üî {row['Feature_2']}: Correla√ß√£o {strength} {direction} ({corr_val:.3f})\")\n\n# An√°lise de multicolinearidade\nprint(f\"\\n‚ö†Ô∏è Poss√≠veis problemas de multicolinearidade (|r| > 0.8):\")\nhigh_corr = correlation_df[correlation_df['Magnitude'] > 0.8]\nif len(high_corr) > 0:\n    for _, row in high_corr.iterrows():\n        print(f\"‚Ä¢ {row['Feature_1']} ‚Üî {row['Feature_2']}: {row['Correla√ß√£o']:.3f}\")\nelse:\n    print(\"‚úÖ Nenhuma correla√ß√£o extremamente alta detectada\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176ecb0",
   "metadata": {},
   "outputs": [],
   "source": "# 8. AN√ÅLISE BIVARIADA - SCATTER PLOTS\n\nprint(\"üé® Criando scatter plots para visualizar rela√ß√µes entre features...\")\n\n# Pairplot com distin√ß√£o por classe\ng = sns.pairplot(data=df[numeric_cols + ['label']], \n                 hue='label', \n                 plot_kws={'alpha': 0.6, 's': 30},\n                 diag_kind='hist',\n                 palette=['skyblue', 'salmon'])\n\n# Personalizar o plot\ng.fig.suptitle('üîç An√°lise Bivariada - Rela√ß√µes entre Features por Classe', \n               y=1.02, fontsize=16, fontweight='bold')\n\n# Adicionar legendas personalizadas\nfor ax in g.axes.flat:\n    if ax.legend_:\n        ax.legend(labels=['Normal', 'Ataque'], loc='best')\n\nplt.show()\n\n# An√°lise de separabilidade das classes\nprint(\"\\nüéØ An√°lise de separabilidade entre classes:\")\nprint(\"Esta visualiza√ß√£o ajuda a identificar:\")\nprint(\"‚Ä¢ Features que separam bem as classes (pontos bem agrupados por cor)\")\nprint(\"‚Ä¢ Rela√ß√µes lineares ou n√£o-lineares entre features\")\nprint(\"‚Ä¢ Poss√≠veis clusters nos dados\")\nprint(\"‚Ä¢ Outliers que podem afetar o modelo\")\n\n# Scatter plots individuais mais detalhados para as top 3 correla√ß√µes\nif len(correlation_df) >= 3:\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    for i in range(3):\n        row = correlation_df.iloc[i]\n        feature1, feature2 = row['Feature_1'], row['Feature_2']\n        \n        # Scatter plot colorido por classe\n        normal_data = df[df['label'] == 0]\n        attack_data = df[df['label'] == 1]\n        \n        axes[i].scatter(normal_data[feature1], normal_data[feature2], \n                       alpha=0.6, c='skyblue', label='Normal', s=30)\n        axes[i].scatter(attack_data[feature1], attack_data[feature2], \n                       alpha=0.6, c='salmon', label='Ataque', s=30)\n        \n        axes[i].set_xlabel(feature1)\n        axes[i].set_ylabel(feature2)\n        axes[i].set_title(f'{feature1} vs {feature2}\\n(r = {row[\"Correla√ß√£o\"]:.3f})')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee218304",
   "metadata": {},
   "outputs": [],
   "source": "# 9. DETEC√á√ÉO E AN√ÅLISE DE OUTLIERS\n\nprint(\"üîç Detectando e analisando outliers nos dados...\")\n\ndef detect_outliers_iqr(data, column):\n    \"\"\"Detecta outliers usando o m√©todo IQR (Interquartile Range)\"\"\"\n    Q1 = data[column].quantile(0.25)\n    Q3 = data[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n    return outliers, lower_bound, upper_bound\n\n# An√°lise de outliers por feature\noutlier_summary = []\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    outliers, lower, upper = detect_outliers_iqr(df, col)\n    outlier_count = len(outliers)\n    outlier_pct = (outlier_count / len(df)) * 100\n    \n    # Armazenar informa√ß√µes do outlier\n    outlier_summary.append({\n        'Feature': col,\n        'Total_Outliers': outlier_count,\n        'Percentual_Outliers': outlier_pct,\n        'Normal_Outliers': len(outliers[outliers['label'] == 0]),\n        'Attack_Outliers': len(outliers[outliers['label'] == 1]),\n        'Lower_Bound': lower,\n        'Upper_Bound': upper\n    })\n    \n    # Visualiza√ß√£o\n    axes[i].boxplot([df[df['label'] == 0][col], df[df['label'] == 1][col]], \n                   labels=['Normal', 'Ataque'], patch_artist=True)\n    \n    # Destacar outliers\n    normal_outliers = outliers[outliers['label'] == 0][col]\n    attack_outliers = outliers[outliers['label'] == 1][col]\n    \n    if len(normal_outliers) > 0:\n        axes[i].scatter([1] * len(normal_outliers), normal_outliers, \n                       color='blue', alpha=0.6, s=20, label=f'Outliers Normal ({len(normal_outliers)})')\n    if len(attack_outliers) > 0:\n        axes[i].scatter([2] * len(attack_outliers), attack_outliers, \n                       color='red', alpha=0.6, s=20, label=f'Outliers Ataque ({len(attack_outliers)})')\n    \n    axes[i].set_title(f'üì¶ {col}\\nOutliers: {outlier_count} ({outlier_pct:.1f}%)')\n    axes[i].grid(True, alpha=0.3)\n    if len(normal_outliers) > 0 or len(attack_outliers) > 0:\n        axes[i].legend()\n\n# Remove subplot extra\nif len(numeric_cols) < len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# Resumo dos outliers\noutlier_df = pd.DataFrame(outlier_summary)\nprint(\"\\nüìä Resumo de Outliers por Feature:\")\ndisplay(outlier_df.round(2))\n\n# An√°lise mais detalhada\nprint(\"\\nüîç An√°lise detalhada dos outliers:\")\ntotal_outliers = outlier_df['Total_Outliers'].sum()\nprint(f\"‚Ä¢ Total de outliers detectados: {total_outliers}\")\nprint(f\"‚Ä¢ Percentual m√©dio de outliers: {outlier_df['Percentual_Outliers'].mean():.2f}%\")\n\nmost_outliers = outlier_df.loc[outlier_df['Total_Outliers'].idxmax()]\nprint(f\"‚Ä¢ Feature com mais outliers: {most_outliers['Feature']} ({most_outliers['Total_Outliers']} outliers)\")\n\n# Distribui√ß√£o de outliers por classe\ntotal_normal_outliers = outlier_df['Normal_Outliers'].sum()\ntotal_attack_outliers = outlier_df['Attack_Outliers'].sum()\nprint(f\"‚Ä¢ Outliers em tr√°fego normal: {total_normal_outliers}\")\nprint(f\"‚Ä¢ Outliers em tr√°fego de ataque: {total_attack_outliers}\")\n\n# An√°lise de outliers extremos (Z-score > 3)\nprint(\"\\n‚ö†Ô∏è Outliers extremos (Z-score > 3):\")\nextreme_outliers_found = False\n\nfor col in numeric_cols:\n    z_scores = np.abs(stats.zscore(df[col].dropna()))\n    extreme_outliers = df[z_scores > 3]\n    \n    if len(extreme_outliers) > 0:\n        extreme_outliers_found = True\n        normal_extreme = len(extreme_outliers[extreme_outliers['label'] == 0])\n        attack_extreme = len(extreme_outliers[extreme_outliers['label'] == 1])\n        print(f\"‚Ä¢ {col}: {len(extreme_outliers)} outliers extremos (Normal: {normal_extreme}, Ataque: {attack_extreme})\")\n\nif not extreme_outliers_found:\n    print(\"‚úÖ Nenhum outlier extremo detectado\")\n\nprint(\"\\nüí° Recomenda√ß√µes:\")\nhigh_outlier_features = outlier_df[outlier_df['Percentual_Outliers'] > 10]\nif len(high_outlier_features) > 0:\n    print(\"‚ö†Ô∏è Features com muitos outliers (>10%):\")\n    for _, row in high_outlier_features.iterrows():\n        print(f\"   ‚Ä¢ {row['Feature']}: {row['Percentual_Outliers']:.1f}% - considere transforma√ß√£o ou remo√ß√£o\")\nelse:\n    print(\"‚úÖ Percentual de outliers aceit√°vel em todas as features\")"
  },
  {
   "cell_type": "markdown",
   "id": "lgep6ps754",
   "source": "# 10. CONCLUS√ïES E INSIGHTS\n\n## üéØ Principais Descobertas da EDA\n\n### Estrutura dos Dados\n- **Tamanho do dataset**: N√∫mero total de fluxos e features analisadas\n- **Qualidade dos dados**: Presen√ßa de valores ausentes e tipos de dados\n- **Balanceamento**: Distribui√ß√£o entre tr√°fego normal e de ataque\n\n### Features Mais Discriminativas\n- **Bytes**: Diferen√ßas no volume de dados transferidos\n- **Pacotes**: N√∫mero de pacotes por fluxo\n- **Dura√ß√£o**: Tempo de dura√ß√£o dos fluxos\n- **Inter-arrival times**: Padr√µes temporais entre pacotes\n\n### Padr√µes Identificados\n- **Tr√°fego Normal**: Caracter√≠sticas t√≠picas observadas\n- **Tr√°fego de Ataque**: Padr√µes an√¥malos detectados\n- **Correla√ß√µes**: Rela√ß√µes importantes entre features\n\n### Outliers e Anomalias\n- **Preval√™ncia**: Percentual de outliers em cada feature\n- **Distribui√ß√£o**: Como os outliers se distribuem entre as classes\n- **Impacto**: Poss√≠vel influ√™ncia nos modelos de ML\n\n## üìà Recomenda√ß√µes para Modelagem\n\n1. **Pr√©-processamento**:\n   - Normaliza√ß√£o/padroniza√ß√£o das features\n   - Tratamento de outliers (remo√ß√£o ou transforma√ß√£o)\n   - Engenharia de features baseada nas correla√ß√µes encontradas\n\n2. **Sele√ß√£o de Features**:\n   - Priorizar features com maior poder discriminativo\n   - Considerar remo√ß√£o de features altamente correlacionadas\n\n3. **Valida√ß√£o**:\n   - Aten√ß√£o ao balanceamento das classes\n   - Uso de m√©tricas apropriadas para datasets desbalanceados\n   - Valida√ß√£o cruzada estratificada\n\n4. **Modelos Sugeridos**:\n   - Random Forest (robusto a outliers)\n   - SVM (bom para separa√ß√£o de classes)\n   - Gradient Boosting (captura rela√ß√µes complexas)\n\n## üîç Pr√≥ximos Passos\n\n1. **Feature Engineering**: Criar novas features baseadas nos insights\n2. **Modelagem**: Treinar e comparar diferentes algoritmos\n3. **Valida√ß√£o**: Avaliar performance com m√©tricas apropriadas\n4. **Interpretabilidade**: Analisar import√¢ncia das features nos modelos",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}