{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f2f95f",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria de Dados (EDA) - Detec√ß√£o de Ataques de Rede\n",
    "\n",
    "## üì° Sobre a Coleta dos Dados\n",
    "\n",
    "**Este dataset foi coletado manualmente em ambiente controlado para garantir a fidelidade dos cen√°rios de rede:**\n",
    "\n",
    "### üî¨ Metodologia de Coleta\n",
    "- **Ambiente**: Rede local controlada para reprodu√ß√£o experimental\n",
    "- **Ferramentas**: Sniffers de rede (Wireshark, tcpdump) para captura de tr√°fego\n",
    "- **Tr√°fego Normal**: Capturado durante uso t√≠pico de rede (navega√ß√£o, downloads, comunica√ß√£o)\n",
    "- **Tr√°fego de Ataque**: Ataques executados manualmente para simular cen√°rios reais\n",
    "\n",
    "### ‚öôÔ∏è Cen√°rios Reproduzidos\n",
    "- **Ataques Manuais**: Executados por especialistas para garantir padr√µes realistas\n",
    "- **Condi√ß√µes Controladas**: Ambiente isolado para evitar interfer√™ncias\n",
    "- **Diversidade de Ataques**: M√∫ltiplos tipos de ataques para variedade no dataset\n",
    "- **Reprodutibilidade**: Metodologia documentada para replica√ß√£o do experimento\n",
    "\n",
    "### üéØ Objetivo\n",
    "Criar um dataset representativo que reflita padr√µes reais de tr√°fego de rede, permitindo o desenvolvimento de modelos de detec√ß√£o de intrus√£o eficazes em ambientes similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c357791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√£o de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üìä Iniciando An√°lise Explorat√≥ria de Dados\")\n",
    "print(\"üî¨ Dataset: Tr√°fego de rede coletado manualmente em ambiente controlado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARREGAMENTO E VIS√ÉO GERAL DOS DADOS\n",
    "\n",
    "# Carregamento dos dados processados\n",
    "df = pd.read_csv('../data/processed/flows.csv')\n",
    "\n",
    "print(f\"‚úÖ Dados carregados com sucesso!\")\n",
    "print(f\"üìà Dimens√µes do dataset: {df.shape[0]} fluxos, {df.shape[1]} features\")\n",
    "print(f\"üîç Primeiras linhas:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ESTRUTURA E TIPOS DE DADOS\n",
    "\n",
    "print(\"üî¨ Informa√ß√µes sobre os dados:\")\n",
    "print(f\"üìä Formato: {df.shape}\")\n",
    "print(f\"üíæ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print()\n",
    "\n",
    "print(\"üìã Tipos de dados:\")\n",
    "display(df.dtypes.to_frame('Tipo'))\n",
    "print()\n",
    "\n",
    "print(\"‚ùå Valores ausentes:\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing,\n",
    "    'Percentual (%)': missing_pct\n",
    "}).round(2)\n",
    "display(missing_df[missing_df['Valores Ausentes'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1022df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ESTAT√çSTICAS DESCRITIVAS E TESTES DE HIP√ìTESE\n",
    "\n",
    "print(\"üìà Resumo estat√≠stico das features num√©ricas:\")\n",
    "numeric_cols = ['bytes', 'pkts', 'duration', 'iat_mean', 'iat_std']\n",
    "desc_stats = df[numeric_cols + ['label']].describe()\n",
    "display(desc_stats.round(4))\n",
    "\n",
    "print()\n",
    "print(\"üéØ Estat√≠sticas por classe (Normal vs Ataque):\")\n",
    "stats_by_label = df.groupby('label')[numeric_cols].describe()\n",
    "display(stats_by_label.round(4))\n",
    "\n",
    "# Consumo de mem√≥ria\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nüíæ Consumo de mem√≥ria do DataFrame: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Testes de hip√≥tese para comparar distribui√ß√µes entre classes\n",
    "print(\"\\nüî¨ Testes Estat√≠sticos (Normal vs Ataque):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from scipy.stats import mannwhitneyu, ks_2samp\n",
    "\n",
    "hypothesis_results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    normal_data = df[df['label'] == 0][col].dropna()\n",
    "    attack_data = df[df['label'] == 1][col].dropna()\n",
    "    \n",
    "    # Teste de Mann-Whitney U (n√£o param√©trico)\n",
    "    mw_stat, mw_p = mannwhitneyu(normal_data, attack_data, alternative='two-sided')\n",
    "    \n",
    "    # Teste de Kolmogorov-Smirnov (distribui√ß√µes)\n",
    "    ks_stat, ks_p = ks_2samp(normal_data, attack_data)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(normal_data) - 1) * normal_data.var() + \n",
    "                          (len(attack_data) - 1) * attack_data.var()) / \n",
    "                         (len(normal_data) + len(attack_data) - 2))\n",
    "    cohens_d = (attack_data.mean() - normal_data.mean()) / pooled_std\n",
    "    \n",
    "    hypothesis_results.append({\n",
    "        'Feature': col,\n",
    "        'Mann_Whitney_U': mw_stat,\n",
    "        'MW_p_value': mw_p,\n",
    "        'MW_Significativo': 'Sim' if mw_p < 0.05 else 'N√£o',\n",
    "        'KS_statistic': ks_stat,\n",
    "        'KS_p_value': ks_p,\n",
    "        'KS_Significativo': 'Sim' if ks_p < 0.05 else 'N√£o',\n",
    "        'Cohens_d': cohens_d,\n",
    "        'Effect_Size': 'Pequeno' if abs(cohens_d) < 0.5 else 'M√©dio' if abs(cohens_d) < 0.8 else 'Grande'\n",
    "    })\n",
    "\n",
    "hypothesis_df = pd.DataFrame(hypothesis_results)\n",
    "display(hypothesis_df.round(4))\n",
    "\n",
    "print(\"\\nüîç Interpreta√ß√£o dos Testes:\")\n",
    "print(\"‚Ä¢ Mann-Whitney U: Testa se as medianas s√£o diferentes (n√£o param√©trico)\")\n",
    "print(\"‚Ä¢ Kolmogorov-Smirnov: Testa se as distribui√ß√µes s√£o diferentes\")\n",
    "print(\"‚Ä¢ Cohen's d: Mede o tamanho do efeito (diferen√ßa padronizada)\")\n",
    "print(\"  - |d| < 0.5: Efeito pequeno\")\n",
    "print(\"  - 0.5 ‚â§ |d| < 0.8: Efeito m√©dio\") \n",
    "print(\"  - |d| ‚â• 0.8: Efeito grande\")\n",
    "\n",
    "# Resumo das features mais discriminativas\n",
    "significant_features = hypothesis_df[\n",
    "    (hypothesis_df['MW_Significativo'] == 'Sim') & \n",
    "    (hypothesis_df['KS_Significativo'] == 'Sim')\n",
    "]['Feature'].tolist()\n",
    "\n",
    "print(f\"\\n‚≠ê Features estatisticamente significativas: {len(significant_features)}/{len(numeric_cols)}\")\n",
    "for feature in significant_features:\n",
    "    effect = hypothesis_df[hypothesis_df['Feature'] == feature]['Effect_Size'].iloc[0]\n",
    "    cohens = hypothesis_df[hypothesis_df['Feature'] == feature]['Cohens_d'].iloc[0]\n",
    "    print(f\"  ‚Ä¢ {feature}: {effect} efeito (d = {cohens:.3f})\")\n",
    "\n",
    "if len(significant_features) < len(numeric_cols):\n",
    "    non_significant = [f for f in numeric_cols if f not in significant_features]\n",
    "    print(f\"\\n‚ö†Ô∏è Features sem diferen√ßa significativa: {non_significant}\")\n",
    "    print(\"   Considere remov√™-las ou combin√°-las para melhorar o modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d7ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. AN√ÅLISE DE BALANCEAMENTO DAS CLASSES\n",
    "\n",
    "print(\"‚öñÔ∏è Distribui√ß√£o das classes:\")\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "label_props = df['label'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "balance_df = pd.DataFrame({\n",
    "    'Classe': ['Normal (0)', 'Ataque (1)'],\n",
    "    'Quantidade': label_counts.values,\n",
    "    'Propor√ß√£o (%)': (label_props.values * 100).round(2)\n",
    "})\n",
    "\n",
    "display(balance_df)\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "balance_df.plot(x='Classe', y='Quantidade', kind='bar', ax=ax1, color=['skyblue', 'salmon'])\n",
    "ax1.set_title('üìä Distribui√ß√£o Absoluta das Classes')\n",
    "ax1.set_xlabel('Classe')\n",
    "ax1.set_ylabel('N√∫mero de Fluxos')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "ax2.pie(balance_df['Quantidade'], labels=balance_df['Classe'], autopct='%1.1f%%', \n",
    "        colors=['skyblue', 'salmon'], startangle=90)\n",
    "ax2.set_title('ü•ß Propor√ß√£o das Classes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise de balanceamento\n",
    "ratio = label_counts.iloc[1] / label_counts.iloc[0]\n",
    "print(f\"üìä Propor√ß√£o Ataque/Normal: {ratio:.3f}\")\n",
    "if ratio < 0.1:\n",
    "    print(\"‚ö†Ô∏è  Dataset muito desbalanceado - considere t√©cnicas de balanceamento\")\n",
    "elif ratio < 0.5:\n",
    "    print(\"‚ö†Ô∏è  Dataset moderadamente desbalanceado\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset relativamente balanceado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DISTRIBUI√á√ïES UNIVARIADAS E AN√ÅLISE DE TRANSFORMA√á√ïES\n",
    "\n",
    "print(\"üìä Analisando distribui√ß√µes das features num√©ricas...\")\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "transformation_results = []\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    # Plot original\n",
    "    axes[i*2].hist(df[col], bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Estat√≠sticas da distribui√ß√£o original\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    skew_val = df[col].skew()\n",
    "    \n",
    "    # Linhas de refer√™ncia\n",
    "    axes[i*2].axvline(mean_val, color='red', linestyle='--', label=f'M√©dia: {mean_val:.2f}')\n",
    "    axes[i*2].axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')\n",
    "    \n",
    "    axes[i*2].set_title(f'üìà {col} (Original)\\nSkew: {skew_val:.2f}')\n",
    "    axes[i*2].set_xlabel(col)\n",
    "    axes[i*2].set_ylabel('Densidade')\n",
    "    axes[i*2].legend()\n",
    "    axes[i*2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Teste de normalidade original\n",
    "    _, p_value_orig = stats.normaltest(df[col].dropna())\n",
    "    normality_orig = \"Normal\" if p_value_orig > 0.05 else \"N√£o Normal\"\n",
    "    \n",
    "    # Aplicar transforma√ß√£o se skew > 1\n",
    "    best_transformation = \"Nenhuma\"\n",
    "    transformed_data = df[col].copy()\n",
    "    skew_transformed = skew_val\n",
    "    normality_transformed = normality_orig\n",
    "    \n",
    "    if abs(skew_val) > 1:\n",
    "        # Testar diferentes transforma√ß√µes\n",
    "        transformations = {}\n",
    "        \n",
    "        # Log transform (apenas para valores positivos)\n",
    "        if (df[col] > 0).all():\n",
    "            log_data = np.log1p(df[col])  # log(1+x) para evitar log(0)\n",
    "            transformations['Log(1+x)'] = log_data\n",
    "        \n",
    "        # Square root transform (apenas para valores n√£o negativos)\n",
    "        if (df[col] >= 0).all():\n",
    "            sqrt_data = np.sqrt(df[col])\n",
    "            transformations['Sqrt'] = sqrt_data\n",
    "        \n",
    "        # Box-Cox transform (apenas para valores positivos)\n",
    "        if (df[col] > 0).all():\n",
    "            try:\n",
    "                boxcox_data, lambda_param = boxcox(df[col])\n",
    "                transformations[f'Box-Cox(Œª={lambda_param:.3f})'] = boxcox_data\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Yeo-Johnson transform (aceita valores negativos)\n",
    "        try:\n",
    "            pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "            yj_data = pt.fit_transform(df[col].values.reshape(-1, 1)).flatten()\n",
    "            transformations['Yeo-Johnson'] = yj_data\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Escolher a melhor transforma√ß√£o (menor skew absoluto)\n",
    "        if transformations:\n",
    "            best_skew = float('inf')\n",
    "            for transform_name, transform_data in transformations.items():\n",
    "                current_skew = abs(stats.skew(transform_data))\n",
    "                if current_skew < best_skew:\n",
    "                    best_skew = current_skew\n",
    "                    best_transformation = transform_name\n",
    "                    transformed_data = transform_data\n",
    "                    skew_transformed = stats.skew(transform_data)\n",
    "    \n",
    "    # Plot transformado\n",
    "    axes[i*2+1].hist(transformed_data, bins=50, alpha=0.7, density=True, \n",
    "                     color='lightcoral', edgecolor='black')\n",
    "    \n",
    "    mean_trans = transformed_data.mean()\n",
    "    median_trans = transformed_data.median()\n",
    "    \n",
    "    axes[i*2+1].axvline(mean_trans, color='red', linestyle='--', label=f'M√©dia: {mean_trans:.2f}')\n",
    "    axes[i*2+1].axvline(median_trans, color='green', linestyle='--', label=f'Mediana: {median_trans:.2f}')\n",
    "    \n",
    "    # Teste de normalidade transformado\n",
    "    if best_transformation != \"Nenhuma\":\n",
    "        _, p_value_trans = stats.normaltest(transformed_data.dropna())\n",
    "        normality_transformed = \"Normal\" if p_value_trans > 0.05 else \"N√£o Normal\"\n",
    "    \n",
    "    axes[i*2+1].set_title(f'üîÑ {col} ({best_transformation})\\nSkew: {skew_transformed:.2f}')\n",
    "    axes[i*2+1].set_xlabel(f'{col} (transformado)')\n",
    "    axes[i*2+1].set_ylabel('Densidade')\n",
    "    axes[i*2+1].legend()\n",
    "    axes[i*2+1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    transformation_results.append({\n",
    "        'Feature': col,\n",
    "        'Skew_Original': skew_val,\n",
    "        'Normalidade_Original': normality_orig,\n",
    "        'Transforma√ß√£o_Recomendada': best_transformation,\n",
    "        'Skew_Transformado': skew_transformed,\n",
    "        'Normalidade_Transformada': normality_transformed,\n",
    "        'Melhoria_Skew': abs(skew_val) - abs(skew_transformed)\n",
    "    })\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo das transforma√ß√µes\n",
    "transformation_df = pd.DataFrame(transformation_results)\n",
    "print(\"\\nüîÑ Resumo das Transforma√ß√µes Recomendadas:\")\n",
    "display(transformation_df.round(3))\n",
    "\n",
    "print(\"\\nüí° Recomenda√ß√µes para Pr√©-processamento:\")\n",
    "high_skew_features = transformation_df[abs(transformation_df['Skew_Original']) > 1]\n",
    "if len(high_skew_features) > 0:\n",
    "    print(\"‚ö†Ô∏è Features com alta assimetria (|skew| > 1):\")\n",
    "    for _, row in high_skew_features.iterrows():\n",
    "        improvement = \"‚úÖ Melhorou\" if row['Melhoria_Skew'] > 0.5 else \"‚ö†Ô∏è Pouca melhoria\"\n",
    "        print(f\"  ‚Ä¢ {row['Feature']}: {row['Transforma√ß√£o_Recomendada']} - {improvement}\")\n",
    "        print(f\"    Skew: {row['Skew_Original']:.2f} ‚Üí {row['Skew_Transformado']:.2f}\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas as features t√™m assimetria aceit√°vel\")\n",
    "\n",
    "print(\"\\nüéØ Estrat√©gia para Autoencoder:\")\n",
    "print(\"‚Ä¢ Features com skew > 2: Aplicar transforma√ß√£o antes da normaliza√ß√£o\")\n",
    "print(\"‚Ä¢ Features normalizadas: StandardScaler ou MinMaxScaler\")\n",
    "print(\"‚Ä¢ Manter transforma√ß√µes invers√≠veis para interpreta√ß√£o dos resultados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AN√ÅLISE COMPARATIVA POR CLASSE (NORMAL VS ATAQUE)\n",
    "\n",
    "print(\"üéØ Comparando distribui√ß√µes entre classes Normal e Ataque...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    # Boxplot comparativo\n",
    "    box_data = [df[df['label'] == 0][col].dropna(), df[df['label'] == 1][col].dropna()]\n",
    "    box = axes[i].boxplot(box_data, labels=['Normal', 'Ataque'], patch_artist=True)\n",
    "    \n",
    "    # Colorir as caixas\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    for patch, color in zip(box['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    axes[i].set_title(f'üì¶ {col} por Classe')\n",
    "    axes[i].set_ylabel(col)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Estat√≠sticas comparativas\n",
    "    normal_mean = df[df['label'] == 0][col].mean()\n",
    "    attack_mean = df[df['label'] == 1][col].mean()\n",
    "    \n",
    "    # Teste t para diferen√ßa de m√©dias\n",
    "    _, p_value = stats.ttest_ind(\n",
    "        df[df['label'] == 0][col].dropna(), \n",
    "        df[df['label'] == 1][col].dropna()\n",
    "    )\n",
    "    \n",
    "    significance = \"Significativa\" if p_value < 0.05 else \"N√£o Significativa\"\n",
    "    \n",
    "    axes[i].text(0.02, 0.98, \n",
    "                f'Normal: {normal_mean:.2f}\\nAtaque: {attack_mean:.2f}\\nDif: {significance}', \n",
    "                transform=axes[i].transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "# Remove o subplot extra\n",
    "if len(numeric_cols) < len(axes):\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise quantitativa das diferen√ßas\n",
    "print(\"\\nüìä Resumo das diferen√ßas entre classes:\")\n",
    "comparison_stats = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    normal_data = df[df['label'] == 0][col]\n",
    "    attack_data = df[df['label'] == 1][col]\n",
    "    \n",
    "    _, p_value = stats.ttest_ind(normal_data.dropna(), attack_data.dropna())\n",
    "    effect_size = (attack_data.mean() - normal_data.mean()) / df[col].std()\n",
    "    \n",
    "    comparison_stats.append({\n",
    "        'Feature': col,\n",
    "        'Normal_Mean': normal_data.mean(),\n",
    "        'Attack_Mean': attack_data.mean(),\n",
    "        'Diferen√ßa_%': ((attack_data.mean() - normal_data.mean()) / normal_data.mean() * 100),\n",
    "        'P_Value': p_value,\n",
    "        'Effect_Size': effect_size\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_stats)\n",
    "display(comparison_df.round(4))\n",
    "\n",
    "print(\"\\nüîç Features mais discriminativas (por tamanho do efeito):\")\n",
    "top_features = comparison_df.reindex(comparison_df['Effect_Size'].abs().sort_values(ascending=False).index)\n",
    "for _, row in top_features.head(3).iterrows():\n",
    "    print(f\"‚Ä¢ {row['Feature']}: Effect Size = {row['Effect_Size']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01905fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. AN√ÅLISE DE CORRELA√á√ïES E MULTICOLINEARIDADE\n",
    "\n",
    "print(\"üîó Analisando correla√ß√µes entre features...\")\n",
    "\n",
    "# Matriz de correla√ß√£o\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "# Visualiza√ß√£o da matriz de correla√ß√£o\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Heatmap principal\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=ax1)\n",
    "ax1.set_title('üå°Ô∏è Matriz de Correla√ß√£o (Pearson)')\n",
    "\n",
    "# Heatmap apenas das correla√ß√µes fortes (|r| > 0.5)\n",
    "strong_corr = corr_matrix.copy()\n",
    "strong_corr[abs(strong_corr) < 0.5] = 0\n",
    "sns.heatmap(strong_corr, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=ax2)\n",
    "ax2.set_title('üî• Correla√ß√µes Fortes (|r| ‚â• 0.5)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identificar correla√ß√µes mais fortes\n",
    "print(\"\\nüìà Correla√ß√µes mais fortes entre features:\")\n",
    "# Criar matriz triangular superior para evitar duplicatas\n",
    "upper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "correlation_pairs = []\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if upper_triangle[i, j]:\n",
    "            feature1 = corr_matrix.columns[i]\n",
    "            feature2 = corr_matrix.columns[j]\n",
    "            correlation = corr_matrix.iloc[i, j]\n",
    "            correlation_pairs.append({\n",
    "                'Feature_1': feature1,\n",
    "                'Feature_2': feature2,\n",
    "                'Correla√ß√£o': correlation,\n",
    "                'Magnitude': abs(correlation)\n",
    "            })\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_pairs)\n",
    "correlation_df = correlation_df.sort_values('Magnitude', ascending=False)\n",
    "\n",
    "print(\"Top 5 correla√ß√µes mais fortes:\")\n",
    "display(correlation_df.head().round(3))\n",
    "\n",
    "# An√°lise cr√≠tica de multicolinearidade\n",
    "print(\"\\n‚ö†Ô∏è An√°lise Cr√≠tica de Multicolinearidade:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# VIF (Variance Inflation Factor)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Preparar dados para VIF (remover valores infinitos/nulos)\n",
    "X_vif = df[numeric_cols].copy()\n",
    "X_vif = X_vif.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "if len(X_vif) > 0:\n",
    "    # Calcular VIF para cada feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X_vif.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) \n",
    "                       for i in range(len(X_vif.columns))]\n",
    "    vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "    \n",
    "    print(\"üìä Variance Inflation Factor (VIF):\")\n",
    "    display(vif_data.round(2))\n",
    "    \n",
    "    print(\"\\nüîç Interpreta√ß√£o do VIF:\")\n",
    "    print(\"‚Ä¢ VIF < 5: Multicolinearidade baixa\")\n",
    "    print(\"‚Ä¢ 5 ‚â§ VIF < 10: Multicolinearidade moderada\")\n",
    "    print(\"‚Ä¢ VIF ‚â• 10: Multicolinearidade alta (problema)\")\n",
    "    \n",
    "    # Identificar features problem√°ticas\n",
    "    high_vif = vif_data[vif_data['VIF'] >= 10]\n",
    "    moderate_vif = vif_data[(vif_data['VIF'] >= 5) & (vif_data['VIF'] < 10)]\n",
    "    \n",
    "    if len(high_vif) > 0:\n",
    "        print(f\"\\nüö® Features com multicolinearidade alta (VIF ‚â• 10):\")\n",
    "        for _, row in high_vif.iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['Feature']}: VIF = {row['VIF']:.2f}\")\n",
    "        print(\"  ‚Üí Recomenda√ß√£o: Remover uma das features correlacionadas\")\n",
    "    \n",
    "    if len(moderate_vif) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Features com multicolinearidade moderada (5 ‚â§ VIF < 10):\")\n",
    "        for _, row in moderate_vif.iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['Feature']}: VIF = {row['VIF']:.2f}\")\n",
    "        print(\"  ‚Üí Recomenda√ß√£o: Monitorar durante a modelagem\")\n",
    "    \n",
    "    if len(high_vif) == 0 and len(moderate_vif) == 0:\n",
    "        print(\"\\n‚úÖ Todas as features t√™m VIF < 5 - Multicolinearidade aceit√°vel\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå N√£o foi poss√≠vel calcular VIF devido a dados inconsistentes\")\n",
    "\n",
    "# An√°lise de correla√ß√µes extremas (|r| > 0.9)\n",
    "print(f\"\\nüî• Correla√ß√µes Extremas (|r| > 0.9):\")\n",
    "extreme_corr = correlation_df[correlation_df['Magnitude'] > 0.9]\n",
    "\n",
    "if len(extreme_corr) > 0:\n",
    "    print(\"Features com correla√ß√£o quase perfeita:\")\n",
    "    for _, row in extreme_corr.iterrows():\n",
    "        direction = \"positiva\" if row['Correla√ß√£o'] > 0 else \"negativa\"\n",
    "        print(f\"‚Ä¢ {row['Feature_1']} ‚Üî {row['Feature_2']}: {row['Correla√ß√£o']:.3f} ({direction})\")\n",
    "    \n",
    "    print(\"\\nüí° Recomenda√ß√µes para Modelagem:\")\n",
    "    print(\"‚Ä¢ Considere remover uma das features em cada par altamente correlacionado\")\n",
    "    print(\"‚Ä¢ Ou combine-as usando PCA ou m√©dia ponderada\")\n",
    "    print(\"‚Ä¢ Para autoencoders, alta correla√ß√£o pode causar redund√¢ncia no aprendizado\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma correla√ß√£o extrema detectada\")\n",
    "\n",
    "# An√°lise de correla√ß√µes fortes (0.7 ‚â§ |r| < 0.9)\n",
    "strong_corr_pairs = correlation_df[(correlation_df['Magnitude'] >= 0.7) & \n",
    "                                   (correlation_df['Magnitude'] < 0.9)]\n",
    "\n",
    "if len(strong_corr_pairs) > 0:\n",
    "    print(f\"\\nüî∂ Correla√ß√µes Fortes (0.7 ‚â§ |r| < 0.9):\")\n",
    "    for _, row in strong_corr_pairs.iterrows():\n",
    "        direction = \"positiva\" if row['Correla√ß√£o'] > 0 else \"negativa\"\n",
    "        print(f\"‚Ä¢ {row['Feature_1']} ‚Üî {row['Feature_2']}: {row['Correla√ß√£o']:.3f} ({direction})\")\n",
    "\n",
    "# Matriz de correla√ß√£o filtrada para visualiza√ß√£o cr√≠tica\n",
    "if len(extreme_corr) > 0 or len(strong_corr_pairs) > 0:\n",
    "    print(f\"\\nüìä Visualiza√ß√£o das Correla√ß√µes Cr√≠ticas:\")\n",
    "    \n",
    "    # Criar m√°scara para correla√ß√µes cr√≠ticas\n",
    "    critical_corr = corr_matrix.copy()\n",
    "    critical_corr[abs(critical_corr) < 0.7] = 0\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = critical_corr == 0\n",
    "    sns.heatmap(critical_corr, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "                square=True, linewidths=0.5, mask=mask, \n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    plt.title('üéØ Correla√ß√µes Cr√≠ticas (|r| ‚â• 0.7)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interpreta√ß√£o das correla√ß√µes para o contexto de rede\n",
    "print(\"\\nüåê Interpreta√ß√£o para Detec√ß√£o de Ataques:\")\n",
    "for _, row in correlation_df.head(3).iterrows():\n",
    "    corr_val = row['Correla√ß√£o']\n",
    "    if abs(corr_val) >= 0.7:\n",
    "        strength = \"muito forte\"\n",
    "    elif abs(corr_val) >= 0.5:\n",
    "        strength = \"forte\"\n",
    "    elif abs(corr_val) >= 0.3:\n",
    "        strength = \"moderada\"\n",
    "    else:\n",
    "        strength = \"fraca\"\n",
    "    \n",
    "    direction = \"positiva\" if corr_val > 0 else \"negativa\"\n",
    "    print(f\"‚Ä¢ {row['Feature_1']} ‚Üî {row['Feature_2']}: Correla√ß√£o {strength} {direction} ({corr_val:.3f})\")\n",
    "    \n",
    "    # Interpreta√ß√£o contextual\n",
    "    if 'bytes' in [row['Feature_1'], row['Feature_2']] and 'pkts' in [row['Feature_1'], row['Feature_2']]:\n",
    "        print(\"  ‚Üí Fluxos com mais pacotes tendem a transferir mais bytes (esperado)\")\n",
    "    elif 'duration' in [row['Feature_1'], row['Feature_2']]:\n",
    "        print(\"  ‚Üí Dura√ß√£o do fluxo relacionada com volume de dados (esperado)\")\n",
    "    elif 'iat' in row['Feature_1'].lower() or 'iat' in row['Feature_2'].lower():\n",
    "        print(\"  ‚Üí Padr√£o temporal pode indicar comportamento automatizado vs humano\")\n",
    "\n",
    "print(f\"\\nüéØ Recomenda√ß√µes Finais para Sele√ß√£o de Features:\")\n",
    "print(\"‚Ä¢ Priorizar features com baixa correla√ß√£o entre si (< 0.8)\")\n",
    "print(\"‚Ä¢ Manter features com alta discrimina√ß√£o entre classes\")\n",
    "print(\"‚Ä¢ Considerar engenharia de features para reduzir multicolinearidade\")\n",
    "print(\"‚Ä¢ Validar sele√ß√£o final com valida√ß√£o cruzada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9176ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. AN√ÅLISE BIVARIADA E REDU√á√ÉO DE DIMENSIONALIDADE\n",
    "\n",
    "print(\"üé® Criando scatter plots para visualizar rela√ß√µes entre features...\")\n",
    "\n",
    "# Pairplot com distin√ß√£o por classe\n",
    "g = sns.pairplot(data=df[numeric_cols + ['label']], \n",
    "                 hue='label', \n",
    "                 plot_kws={'alpha': 0.6, 's': 30},\n",
    "                 diag_kind='hist',\n",
    "                 palette=['skyblue', 'salmon'])\n",
    "\n",
    "# Personalizar o plot\n",
    "g.fig.suptitle('üîç An√°lise Bivariada - Rela√ß√µes entre Features por Classe', \n",
    "               y=1.02, fontsize=16, fontweight='bold')\n",
    "\n",
    "# Adicionar legendas personalizadas\n",
    "for ax in g.axes.flat:\n",
    "    if ax.legend_:\n",
    "        ax.legend(labels=['Normal', 'Ataque'], loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# An√°lise PCA para visualiza√ß√£o de clusters\n",
    "print(\"\\nüîç An√°lise de Componentes Principais (PCA)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparar dados para PCA\n",
    "X = df[numeric_cols].copy()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Vari√¢ncia explicada\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(f\"üìä Vari√¢ncia explicada por componente:\")\n",
    "for i, var in enumerate(explained_variance):\n",
    "    print(f\"  PC{i+1}: {var:.3f} ({var*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Vari√¢ncia acumulada:\")\n",
    "for i, cum_var in enumerate(cumulative_variance):\n",
    "    print(f\"  PC1-PC{i+1}: {cum_var:.3f} ({cum_var*100:.1f}%)\")\n",
    "\n",
    "# Visualiza√ß√µes PCA\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Vari√¢ncia explicada\n",
    "ax1.bar(range(1, len(explained_variance)+1), explained_variance, alpha=0.7, color='skyblue')\n",
    "ax1.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'ro-', color='red')\n",
    "ax1.set_xlabel('Componente Principal')\n",
    "ax1.set_ylabel('Vari√¢ncia Explicada')\n",
    "ax1.set_title('üìä Vari√¢ncia Explicada por Componente')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(['Acumulada', 'Individual'])\n",
    "\n",
    "# 2. Scatter plot PC1 vs PC2\n",
    "normal_mask = df['label'] == 0\n",
    "attack_mask = df['label'] == 1\n",
    "\n",
    "ax2.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n",
    "           alpha=0.6, c='skyblue', label='Normal', s=30)\n",
    "ax2.scatter(X_pca[attack_mask, 0], X_pca[attack_mask, 1], \n",
    "           alpha=0.6, c='salmon', label='Ataque', s=30)\n",
    "ax2.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
    "ax2.set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
    "ax2.set_title('üéØ Proje√ß√£o PCA (PC1 vs PC2)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loading plot (contribui√ß√£o das features)\n",
    "loadings = pca.components_[:2].T * np.sqrt(pca.explained_variance_[:2])\n",
    "ax3.scatter(loadings[:, 0], loadings[:, 1], alpha=0.7, s=100, color='purple')\n",
    "for i, feature in enumerate(numeric_cols):\n",
    "    ax3.annotate(feature, (loadings[i, 0], loadings[i, 1]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax3.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
    "ax3.set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n",
    "ax3.set_title('üéØ Loading Plot (Contribui√ß√£o das Features)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax3.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. PC1 vs PC3\n",
    "ax4.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 2], \n",
    "           alpha=0.6, c='skyblue', label='Normal', s=30)\n",
    "ax4.scatter(X_pca[attack_mask, 0], X_pca[attack_mask, 2], \n",
    "           alpha=0.6, c='salmon', label='Ataque', s=30)\n",
    "ax4.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n",
    "ax4.set_ylabel(f'PC3 ({explained_variance[2]*100:.1f}%)')\n",
    "ax4.set_title('üéØ Proje√ß√£o PCA (PC1 vs PC3)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lise das componentes principais\n",
    "print(\"\\nüîç Interpreta√ß√£o das Componentes Principais:\")\n",
    "feature_contributions = pd.DataFrame(\n",
    "    pca.components_[:3].T, \n",
    "    columns=[f'PC{i+1}' for i in range(3)], \n",
    "    index=numeric_cols\n",
    ")\n",
    "display(feature_contributions.round(3))\n",
    "\n",
    "print(\"\\nüí° Insights do PCA:\")\n",
    "print(f\"‚Ä¢ PC1 e PC2 explicam {(cumulative_variance[1]*100):.1f}% da vari√¢ncia\")\n",
    "print(f\"‚Ä¢ Para capturar 95% da vari√¢ncia, precisamos de {np.argmax(cumulative_variance >= 0.95) + 1} componentes\")\n",
    "\n",
    "# Avaliar separabilidade das classes no espa√ßo PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(X_pca[:, :2], df['label'])\n",
    "print(f\"‚Ä¢ Silhouette Score (PC1-PC2): {silhouette_avg:.3f}\")\n",
    "\n",
    "if silhouette_avg > 0.5:\n",
    "    print(\"  ‚úÖ Boa separabilidade entre classes no espa√ßo PCA\")\n",
    "elif silhouette_avg > 0.3:\n",
    "    print(\"  ‚ö†Ô∏è Separabilidade moderada entre classes\")\n",
    "else:\n",
    "    print(\"  ‚ùå Baixa separabilidade - classes sobrepostas\")\n",
    "\n",
    "# Scatter plots individuais para as top 3 correla√ß√µes\n",
    "if len(correlation_df) >= 3:\n",
    "    print(\"\\nüîç An√°lise Detalhada das Principais Correla√ß√µes:\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for i in range(3):\n",
    "        row = correlation_df.iloc[i]\n",
    "        feature1, feature2 = row['Feature_1'], row['Feature_2']\n",
    "        \n",
    "        # Scatter plot colorido por classe\n",
    "        normal_data = df[df['label'] == 0]\n",
    "        attack_data = df[df['label'] == 1]\n",
    "        \n",
    "        axes[i].scatter(normal_data[feature1], normal_data[feature2], \n",
    "                       alpha=0.6, c='skyblue', label='Normal', s=30)\n",
    "        axes[i].scatter(attack_data[feature1], attack_data[feature2], \n",
    "                       alpha=0.6, c='salmon', label='Ataque', s=30)\n",
    "        \n",
    "        axes[i].set_xlabel(feature1)\n",
    "        axes[i].set_ylabel(feature2)\n",
    "        axes[i].set_title(f'{feature1} vs {feature2}\\n(r = {row[\"Correla√ß√£o\"]:.3f})')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüéØ Recomenda√ß√µes para Modelagem:\")\n",
    "print(\"‚Ä¢ Use PCA se o autoencoder tiver dificuldades com a dimensionalidade\")\n",
    "print(\"‚Ä¢ As 2-3 primeiras componentes capturam a maior parte da informa√ß√£o\")\n",
    "print(\"‚Ä¢ Considere usar features transformadas para melhor separabilidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee218304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. DETEC√á√ÉO AVAN√áADA DE OUTLIERS\n",
    "\n",
    "print(\"üîç Detectando e analisando outliers com m√∫ltiplos m√©todos...\")\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detecta outliers usando o m√©todo IQR (Interquartile Range)\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detecta outliers usando Z-score\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outlier_indices = data[column].dropna().index[z_scores > threshold]\n",
    "    outliers = data.loc[outlier_indices]\n",
    "    return outliers, threshold\n",
    "\n",
    "def detect_outliers_modified_zscore(data, column, threshold=3.5):\n",
    "    \"\"\"Detecta outliers usando Modified Z-score (mais robusto)\"\"\"\n",
    "    median = data[column].median()\n",
    "    mad = np.median(np.abs(data[column] - median))\n",
    "    modified_z_scores = 0.6745 * (data[column] - median) / mad\n",
    "    outlier_indices = data[np.abs(modified_z_scores) > threshold].index\n",
    "    outliers = data.loc[outlier_indices]\n",
    "    return outliers, threshold\n",
    "\n",
    "# An√°lise comparativa de m√©todos de detec√ß√£o\n",
    "outlier_comparison = []\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    # M√©todo 1: IQR\n",
    "    iqr_outliers, iqr_lower, iqr_upper = detect_outliers_iqr(df, col)\n",
    "    \n",
    "    # M√©todo 2: Z-score\n",
    "    zscore_outliers, zscore_thresh = detect_outliers_zscore(df, col, threshold=3)\n",
    "    \n",
    "    # M√©todo 3: Modified Z-score\n",
    "    mod_zscore_outliers, mod_thresh = detect_outliers_modified_zscore(df, col, threshold=3.5)\n",
    "    \n",
    "    # Interse√ß√£o dos m√©todos (outliers mais robustos)\n",
    "    iqr_indices = set(iqr_outliers.index)\n",
    "    zscore_indices = set(zscore_outliers.index)\n",
    "    mod_zscore_indices = set(mod_zscore_outliers.index)\n",
    "    \n",
    "    # Outliers detectados por pelo menos 2 m√©todos\n",
    "    consensus_outliers = iqr_indices.intersection(zscore_indices).union(\n",
    "        iqr_indices.intersection(mod_zscore_indices)).union(\n",
    "        zscore_indices.intersection(mod_zscore_indices))\n",
    "    \n",
    "    outlier_comparison.append({\n",
    "        'Feature': col,\n",
    "        'IQR_Outliers': len(iqr_outliers),\n",
    "        'IQR_Percent': (len(iqr_outliers) / len(df)) * 100,\n",
    "        'ZScore_Outliers': len(zscore_outliers),\n",
    "        'ZScore_Percent': (len(zscore_outliers) / len(df)) * 100,\n",
    "        'ModZScore_Outliers': len(mod_zscore_outliers),\n",
    "        'ModZScore_Percent': (len(mod_zscore_outliers) / len(df)) * 100,\n",
    "        'Consensus_Outliers': len(consensus_outliers),\n",
    "        'Consensus_Percent': (len(consensus_outliers) / len(df)) * 100,\n",
    "        'Normal_Consensus': len([idx for idx in consensus_outliers if df.loc[idx, 'label'] == 0]),\n",
    "        'Attack_Consensus': len([idx for idx in consensus_outliers if df.loc[idx, 'label'] == 1])\n",
    "    })\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    axes[i].boxplot([df[df['label'] == 0][col], df[df['label'] == 1][col]], \n",
    "                   labels=['Normal', 'Ataque'], patch_artist=True)\n",
    "    \n",
    "    # Destacar outliers consensus\n",
    "    if consensus_outliers:\n",
    "        consensus_data = df.loc[list(consensus_outliers)]\n",
    "        normal_consensus = consensus_data[consensus_data['label'] == 0][col]\n",
    "        attack_consensus = consensus_data[consensus_data['label'] == 1][col]\n",
    "        \n",
    "        if len(normal_consensus) > 0:\n",
    "            axes[i].scatter([1] * len(normal_consensus), normal_consensus, \n",
    "                           color='darkblue', alpha=0.8, s=50, marker='x', \n",
    "                           label=f'Outliers Normal ({len(normal_consensus)})')\n",
    "        if len(attack_consensus) > 0:\n",
    "            axes[i].scatter([2] * len(attack_consensus), attack_consensus, \n",
    "                           color='darkred', alpha=0.8, s=50, marker='x',\n",
    "                           label=f'Outliers Ataque ({len(attack_consensus)})')\n",
    "    \n",
    "    axes[i].set_title(f'üì¶ {col}\\nConsensus Outliers: {len(consensus_outliers)} ({(len(consensus_outliers)/len(df)*100):.1f}%)')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    if consensus_outliers:\n",
    "        axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo comparativo dos m√©todos\n",
    "outlier_df = pd.DataFrame(outlier_comparison)\n",
    "print(\"\\nüìä Compara√ß√£o de M√©todos de Detec√ß√£o de Outliers:\")\n",
    "display(outlier_df.round(2))\n",
    "\n",
    "# An√°lise estat√≠stica dos outliers consensus\n",
    "print(\"\\nüîç An√°lise Detalhada dos Outliers Consensus:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_consensus = outlier_df['Consensus_Outliers'].sum()\n",
    "total_normal_consensus = outlier_df['Normal_Consensus'].sum()\n",
    "total_attack_consensus = outlier_df['Attack_Consensus'].sum()\n",
    "\n",
    "print(f\"‚Ä¢ Total de outliers consensus: {total_consensus}\")\n",
    "print(f\"‚Ä¢ Outliers em tr√°fego normal: {total_normal_consensus} ({(total_normal_consensus/total_consensus*100):.1f}%)\")\n",
    "print(f\"‚Ä¢ Outliers em tr√°fego de ataque: {total_attack_consensus} ({(total_attack_consensus/total_consensus*100):.1f}%)\")\n",
    "\n",
    "# Identificar fluxos que s√£o outliers em m√∫ltiplas features\n",
    "print(\"\\nüéØ Fluxos Outliers em M√∫ltiplas Features:\")\n",
    "outlier_counts_per_flow = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    iqr_outliers, _, _ = detect_outliers_iqr(df, col)\n",
    "    zscore_outliers, _ = detect_outliers_zscore(df, col, threshold=3)\n",
    "    \n",
    "    consensus_indices = set(iqr_outliers.index).intersection(set(zscore_outliers.index))\n",
    "    \n",
    "    for idx in consensus_indices:\n",
    "        outlier_counts_per_flow[idx] = outlier_counts_per_flow.get(idx, 0) + 1\n",
    "\n",
    "# Fluxos que s√£o outliers em m√∫ltiplas features\n",
    "multi_feature_outliers = {k: v for k, v in outlier_counts_per_flow.items() if v >= 2}\n",
    "\n",
    "if multi_feature_outliers:\n",
    "    print(f\"‚Ä¢ Fluxos outliers em 2+ features: {len(multi_feature_outliers)}\")\n",
    "    \n",
    "    # Analisar caracter√≠sticas desses fluxos\n",
    "    extreme_outlier_indices = list(multi_feature_outliers.keys())\n",
    "    extreme_outliers_df = df.loc[extreme_outlier_indices]\n",
    "    \n",
    "    print(\"\\nüìà Caracter√≠sticas dos Outliers Extremos:\")\n",
    "    extreme_stats = extreme_outliers_df.groupby('label')[numeric_cols].agg(['count', 'mean', 'median']).round(2)\n",
    "    display(extreme_stats)\n",
    "    \n",
    "    # Top 5 fluxos mais an√¥malos\n",
    "    sorted_outliers = sorted(multi_feature_outliers.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"\\nüî• Top 5 Fluxos Mais An√¥malos:\")\n",
    "    for idx, count in sorted_outliers:\n",
    "        label_name = \"Ataque\" if df.loc[idx, 'label'] == 1 else \"Normal\"\n",
    "        print(f\"  ‚Ä¢ Fluxo {idx}: Outlier em {count} features ({label_name})\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Nenhum fluxo √© outlier em m√∫ltiplas features\")\n",
    "\n",
    "# An√°lise Z-score extremo (|z| > 4)\n",
    "print(\"\\n‚ö†Ô∏è Outliers Extremos (|Z-score| > 4):\")\n",
    "extreme_found = False\n",
    "\n",
    "for col in numeric_cols:\n",
    "    z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "    extreme_mask = z_scores > 4\n",
    "    \n",
    "    if extreme_mask.any():\n",
    "        extreme_found = True\n",
    "        extreme_indices = df[col].dropna().index[extreme_mask]\n",
    "        normal_extreme = sum(df.loc[extreme_indices, 'label'] == 0)\n",
    "        attack_extreme = sum(df.loc[extreme_indices, 'label'] == 1)\n",
    "        max_zscore = z_scores.max()\n",
    "        \n",
    "        print(f\"‚Ä¢ {col}: {extreme_mask.sum()} outliers extremos (|Z| max: {max_zscore:.2f})\")\n",
    "        print(f\"  Normal: {normal_extreme}, Ataque: {attack_extreme}\")\n",
    "\n",
    "if not extreme_found:\n",
    "    print(\"‚úÖ Nenhum outlier extremo detectado (|Z-score| > 4)\")\n",
    "\n",
    "print(\"\\nüí° Recomenda√ß√µes para Pr√©-processamento:\")\n",
    "high_outlier_features = outlier_df[outlier_df['Consensus_Percent'] > 5]\n",
    "\n",
    "if len(high_outlier_features) > 0:\n",
    "    print(\"‚ö†Ô∏è Features com muitos outliers consensus (>5%):\")\n",
    "    for _, row in high_outlier_features.iterrows():\n",
    "        print(f\"   ‚Ä¢ {row['Feature']}: {row['Consensus_Percent']:.1f}%\")\n",
    "        if row['Attack_Consensus'] > row['Normal_Consensus']:\n",
    "            print(f\"     ‚Üí Outliers majoritariamente em ataques - MANTER para detec√ß√£o\")\n",
    "        else:\n",
    "            print(f\"     ‚Üí Outliers balanceados - considerar transforma√ß√£o ou remo√ß√£o\")\n",
    "else:\n",
    "    print(\"‚úÖ Percentual de outliers aceit√°vel em todas as features\")\n",
    "\n",
    "print(\"\\nüéØ Estrat√©gia para Autoencoder:\")\n",
    "print(\"‚Ä¢ Treinar apenas com dados normais (sem outliers)\")\n",
    "print(\"‚Ä¢ Usar outliers consensus para valida√ß√£o da detec√ß√£o de anomalias\")\n",
    "print(\"‚Ä¢ Manter outliers de ataque para teste do modelo\")\n",
    "print(\"‚Ä¢ Considerar remo√ß√£o apenas de outliers extremos do tr√°fego normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgep6ps754",
   "metadata": {},
   "source": [
    "# 10. CONCLUS√ïES E ROADMAP PARA MODELAGEM\n",
    "\n",
    "## üéØ Principais Descobertas da EDA\n",
    "\n",
    "### Estrutura dos Dados\n",
    "- **Tamanho do dataset**: An√°lise completa de fluxos e features\n",
    "- **Qualidade dos dados**: Verifica√ß√£o de valores ausentes e integridade\n",
    "- **Balanceamento**: Distribui√ß√£o quantificada entre tr√°fego normal e de ataque\n",
    "\n",
    "### Features Mais Discriminativas (Baseado em Testes Estat√≠sticos)\n",
    "- **Signific√¢ncia Estat√≠stica**: Features com diferen√ßas significativas entre classes\n",
    "- **Effect Size**: Medi√ß√£o do impacto discriminativo de cada feature\n",
    "- **Power Analysis**: Identifica√ß√£o das features mais importantes para o modelo\n",
    "\n",
    "### Padr√µes Identificados\n",
    "- **Distribui√ß√µes**: An√°lise de normalidade e assimetria das features\n",
    "- **Transforma√ß√µes**: Recomenda√ß√µes espec√≠ficas para features enviesadas\n",
    "- **Correla√ß√µes**: Mapeamento de rela√ß√µes entre vari√°veis\n",
    "- **Clusters**: Visualiza√ß√£o PCA mostra separabilidade das classes\n",
    "\n",
    "### Outliers e Estrat√©gia de Tratamento\n",
    "- **M√©todos M√∫ltiplos**: IQR, Z-score e Modified Z-score para detec√ß√£o robusta\n",
    "- **Consensus Outliers**: Outliers detectados por m√∫ltiplos m√©todos\n",
    "- **Distribui√ß√£o por Classe**: An√°lise de onde os outliers se concentram\n",
    "\n",
    "## üìà Pipeline de Pr√©-processamento Recomendado\n",
    "\n",
    "### 1. **Limpeza e Prepara√ß√£o**\n",
    "```python\n",
    "# Baseado nos resultados da EDA\n",
    "steps = [\n",
    "    'Verificar valores ausentes (j√° validado)',\n",
    "    'Remover outliers extremos apenas do tr√°fego normal',\n",
    "    'Manter outliers de ataque para valida√ß√£o do modelo'\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. **Transforma√ß√µes de Features**\n",
    "```python\n",
    "# Features com alta assimetria (|skew| > 2)\n",
    "transformations = {\n",
    "    'feature_X': 'Log(1+x)',  # Se aplic√°vel\n",
    "    'feature_Y': 'Box-Cox',   # Se aplic√°vel\n",
    "    'feature_Z': 'Yeo-Johnson'  # Mais robusto\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. **Normaliza√ß√£o**\n",
    "```python\n",
    "# Ap√≥s transforma√ß√µes\n",
    "normalization_strategy = 'StandardScaler'  # Recomendado para autoencoders\n",
    "# Alternativa: MinMaxScaler para features j√° bem distribu√≠das\n",
    "```\n",
    "\n",
    "### 4. **Sele√ß√£o de Features**\n",
    "```python\n",
    "# Baseado nos testes estat√≠sticos\n",
    "selected_features = [\n",
    "    # Features com alta signific√¢ncia estat√≠stica\n",
    "    # Features com baixa correla√ß√£o entre si (< 0.9)\n",
    "    # Features identificadas nos testes de hip√≥tese\n",
    "]\n",
    "```\n",
    "\n",
    "## ü§ñ Estrat√©gia para Autoencoder\n",
    "\n",
    "### **Arquitetura Recomendada**\n",
    "- **Input Layer**: N√∫mero de features selecionadas\n",
    "- **Encoder**: Redu√ß√£o progressiva (ex: input ‚Üí 32 ‚Üí 16 ‚Üí 8)\n",
    "- **Bottleneck**: 3-5 neur√¥nios (baseado na an√°lise PCA)\n",
    "- **Decoder**: Expans√£o sim√©trica (8 ‚Üí 16 ‚Üí 32 ‚Üí input)\n",
    "- **Activation**: ReLU nas camadas ocultas, linear na sa√≠da\n",
    "\n",
    "### **Estrat√©gia de Treinamento**\n",
    "1. **Dados de Treino**: Apenas tr√°fego normal (label=0)\n",
    "2. **Remo√ß√£o de Outliers**: Remover outliers consensus apenas do tr√°fego normal\n",
    "3. **Valida√ß√£o**: Split do tr√°fego normal (80% treino, 20% valida√ß√£o)\n",
    "4. **Teste**: Conjunto misto (normal + ataque) para avaliar detec√ß√£o\n",
    "\n",
    "### **Threshold para Detec√ß√£o**\n",
    "- **Baseline**: Usar erro de reconstru√ß√£o no conjunto de valida√ß√£o normal\n",
    "- **Threshold**: Percentil 95-99 do erro de reconstru√ß√£o no tr√°fego normal\n",
    "- **Valida√ß√£o**: Testar com dados de ataque conhecidos\n",
    "\n",
    "## üîç Pr√≥ximos Passos Detalhados\n",
    "\n",
    "### **Etapa 5: Implementa√ß√£o do Pipeline**\n",
    "1. **Criar classe de pr√©-processamento** com transforma√ß√µes identificadas\n",
    "2. **Implementar autoencoder** com arquitetura baseada na EDA\n",
    "3. **Pipeline scikit-learn** para integra√ß√£o das etapas\n",
    "4. **Valida√ß√£o cruzada** estratificada para robustez\n",
    "\n",
    "### **Etapa 6: Treinamento e Valida√ß√£o**\n",
    "1. **M√©tricas de Avalia√ß√£o**:\n",
    "   - Precis√£o, Recall, F1-Score\n",
    "   - ROC-AUC (objetivo: >0.90)\n",
    "   - Precision-Recall AUC para dados desbalanceados\n",
    "2. **An√°lise de Threshold**:\n",
    "   - Curva ROC para otimiza√ß√£o\n",
    "   - Trade-off entre falsos positivos e falsos negativos\n",
    "\n",
    "### **Etapa 7: Interpretabilidade**\n",
    "1. **Feature Importance**: An√°lise do impacto de cada feature no erro\n",
    "2. **Visualiza√ß√£o**: t-SNE/UMAP do espa√ßo latente\n",
    "3. **Casos de Erro**: An√°lise de falsos positivos/negativos\n",
    "\n",
    "## üìä M√©tricas de Sucesso Esperadas\n",
    "\n",
    "### **Baseline M√≠nimo**\n",
    "- **Accuracy**: >85%\n",
    "- **Precision**: >80% (minimizar falsos positivos)\n",
    "- **Recall**: >90% (detectar a maioria dos ataques)\n",
    "- **F1-Score**: >85%\n",
    "\n",
    "### **Objetivo Ideal**\n",
    "- **ROC-AUC**: >0.95\n",
    "- **PR-AUC**: >0.90\n",
    "- **False Positive Rate**: <5%\n",
    "- **Detection Rate**: >95%\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Valida√ß√£o da Metodologia\n",
    "\n",
    "### üî¨ **Qualidades do Dataset**\n",
    "- **Coleta Manual**: Dados aut√™nticos coletados em ambiente controlado\n",
    "- **Ataques Reais**: Executados por especialistas para garantir realismo\n",
    "- **Ambiente Controlado**: Reduz ru√≠do e garante qualidade dos dados\n",
    "- **Reprodutibilidade**: Metodologia documentada permite replica√ß√£o\n",
    "\n",
    "### ‚ö†Ô∏è **Limita√ß√µes e Considera√ß√µes**\n",
    "- **Escopo**: Aplic√°vel a ambientes similares ao de coleta\n",
    "- **Temporal**: Padr√µes de ataque evoluem - necess√°rio retreinamento peri√≥dico\n",
    "- **Generaliza√ß√£o**: Pode necessitar ajustes para outros tipos de rede\n",
    "\n",
    "### üéØ **Contribui√ß√µes Esperadas**\n",
    "- **Metodologia**: Pipeline completo de detec√ß√£o com autoencoders\n",
    "- **Benchmark**: Baseline para futuros trabalhos na √°rea\n",
    "- **Reprodutibilidade**: C√≥digo e dados para valida√ß√£o por terceiros\n",
    "\n",
    "## üöÄ **Comando para Pr√≥xima Etapa**\n",
    "\n",
    "```bash\n",
    "# Executar ap√≥s conclus√£o da EDA\n",
    "python scripts/train_autoencoder.py --config config/model_config.yaml\n",
    "```\n",
    "\n",
    "**Status**: ‚úÖ EDA Completa - Pronto para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "id": "eyopuq5minn",
   "source": "# 11. EXPORTAR TABELAS E ARTEFATOS PARA REPORTS/\n\nprint(\"üíæ Exportando tabelas-chave para reports/...\")\n\n# Criar diret√≥rio reports se n√£o existir\nimport os\nos.makedirs('../reports', exist_ok=True)\nos.makedirs('../figures', exist_ok=True)\n\n# 1. Exportar resultados dos testes de hip√≥tese\nif 'hypothesis_df' in locals():\n    hypothesis_df.to_csv('../reports/statistical_tests_results.csv', index=False)\n    print(\"‚úÖ Resultados dos testes estat√≠sticos salvos em reports/statistical_tests_results.csv\")\n\n# 2. Exportar resultados das transforma√ß√µes\nif 'transformation_df' in locals():\n    transformation_df.to_csv('../reports/feature_transformations.csv', index=False)\n    print(\"‚úÖ Recomenda√ß√µes de transforma√ß√µes salvas em reports/feature_transformations.csv\")\n\n# 3. Exportar dados de VIF (multicolinearidade)\nif 'vif_data' in locals():\n    vif_data.to_csv('../reports/vif_multicollinearity.csv', index=False)\n    print(\"‚úÖ An√°lise de multicolinearidade (VIF) salva em reports/vif_multicollinearity.csv\")\n\n# 4. Exportar matriz de correla√ß√£o\ncorrelation_matrix = df[numeric_cols].corr()\ncorrelation_matrix.to_csv('../reports/correlation_matrix.csv')\nprint(\"‚úÖ Matriz de correla√ß√£o salva em reports/correlation_matrix.csv\")\n\n# 5. Exportar compara√ß√µes estat√≠sticas entre classes\nif 'comparison_df' in locals():\n    comparison_df.to_csv('../reports/class_comparison_stats.csv', index=False)\n    print(\"‚úÖ Compara√ß√µes estat√≠sticas entre classes salvas em reports/class_comparison_stats.csv\")\n\n# 6. Exportar an√°lise de outliers\nif 'outlier_df' in locals():\n    outlier_df.to_csv('../reports/outlier_analysis.csv', index=False)\n    print(\"‚úÖ An√°lise de outliers salva em reports/outlier_analysis.csv\")\n\n# 7. Exportar contribui√ß√µes das componentes principais\nif 'feature_contributions' in locals():\n    feature_contributions.to_csv('../reports/pca_feature_contributions.csv')\n    print(\"‚úÖ Contribui√ß√µes PCA salvas em reports/pca_feature_contributions.csv\")\n\n# 8. Exportar resumo geral do dataset\ndataset_summary = {\n    'Total_Samples': len(df),\n    'Total_Features': len(numeric_cols),\n    'Normal_Traffic': len(df[df['label'] == 0]),\n    'Attack_Traffic': len(df[df['label'] == 1]),\n    'Missing_Values': df.isnull().sum().sum(),\n    'Memory_Usage_MB': df.memory_usage(deep=True).sum() / 1024**2\n}\n\nsummary_df = pd.DataFrame([dataset_summary])\nsummary_df.to_csv('../reports/dataset_summary.csv', index=False)\nprint(\"‚úÖ Resumo do dataset salvo em reports/dataset_summary.csv\")\n\n# 9. Exportar features selecionadas para o modelo\nselected_features_info = {\n    'All_Features': numeric_cols,\n    'Significant_Features': significant_features if 'significant_features' in locals() else [],\n    'Features_Count': len(numeric_cols),\n    'Significant_Count': len(significant_features) if 'significant_features' in locals() else 0\n}\n\n# Salvar como JSON para facilitar leitura pelo pipeline\nimport json\nwith open('../reports/selected_features.json', 'w') as f:\n    json.dump(selected_features_info, f, indent=2)\nprint(\"‚úÖ Features selecionadas salvas em reports/selected_features.json\")\n\nprint(f\"\\nüìä Total de arquivos exportados: 9\")\nprint(\"üìÅ Todos os artefatos est√£o dispon√≠veis em reports/ para refer√™ncia no artigo\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1gv3c8ror06",
   "source": "# 12. SALVAR VISUALIZA√á√ïES PRINCIPAIS EM FIGURES/\n\nprint(\"üé® Salvando visualiza√ß√µes principais em figures/...\")\n\n# Configura√ß√µes para alta qualidade\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['savefig.dpi'] = 300\nplt.rcParams['savefig.bbox'] = 'tight'\n\n# 1. Salvar distribui√ß√µes transformadas (recria√ß√£o otimizada)\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    if i < len(axes):\n        # Distribui√ß√£o original\n        axes[i].hist(df[col], bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n        axes[i].set_title(f'{col} - Distribui√ß√£o Original\\nSkew: {df[col].skew():.2f}')\n        axes[i].set_xlabel(col)\n        axes[i].set_ylabel('Densidade')\n        axes[i].grid(True, alpha=0.3)\n\n# Remove subplot extra se houver\nif len(numeric_cols) < len(axes):\n    for i in range(len(numeric_cols), len(axes)):\n        fig.delaxes(axes[i])\n\nplt.suptitle('Distribui√ß√µes das Features Num√©ricas', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.savefig('../figures/01_feature_distributions.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"‚úÖ Distribui√ß√µes das features salvas em figures/01_feature_distributions.png\")\n\n# 2. Boxplots comparativos por classe\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    if i < len(axes):\n        box_data = [df[df['label'] == 0][col].dropna(), df[df['label'] == 1][col].dropna()]\n        box = axes[i].boxplot(box_data, labels=['Normal', 'Ataque'], patch_artist=True)\n        \n        # Colorir as caixas\n        colors = ['lightblue', 'lightcoral']\n        for patch, color in zip(box['boxes'], colors):\n            patch.set_facecolor(color)\n        \n        axes[i].set_title(f'{col} por Classe')\n        axes[i].set_ylabel(col)\n        axes[i].grid(True, alpha=0.3)\n\n# Remove subplot extra se houver\nif len(numeric_cols) < len(axes):\n    for i in range(len(numeric_cols), len(axes)):\n        fig.delaxes(axes[i])\n\nplt.suptitle('Compara√ß√£o de Features entre Classes Normal e Ataque', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.savefig('../figures/02_class_comparison_boxplots.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"‚úÖ Boxplots comparativos salvos em figures/02_class_comparison_boxplots.png\")\n\n# 3. Matriz de correla√ß√£o cr√≠tica\ncritical_threshold = 0.5\ncorr_matrix = df[numeric_cols].corr()\ncritical_corr = corr_matrix.copy()\ncritical_corr[abs(critical_corr) < critical_threshold] = 0\n\nplt.figure(figsize=(12, 10))\nmask = critical_corr == 0\nsns.heatmap(critical_corr, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n            square=True, linewidths=0.5, mask=mask, \n            cbar_kws={\"shrink\": .8})\nplt.title(f'Matriz de Correla√ß√µes Cr√≠ticas (|r| ‚â• {critical_threshold})', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('../figures/03_correlation_matrix_critical.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"‚úÖ Matriz de correla√ß√£o cr√≠tica salva em figures/03_correlation_matrix_critical.png\")\n\n# 4. Proje√ß√£o PCA recriada\nif len(df) > 0:\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import StandardScaler\n    \n    X = df[numeric_cols].copy()\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    pca = PCA()\n    X_pca = pca.fit_transform(X_scaled)\n    \n    explained_variance = pca.explained_variance_ratio_\n    \n    # Subplot com m√∫ltiplas visualiza√ß√µes PCA\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Vari√¢ncia explicada\n    ax1.bar(range(1, len(explained_variance)+1), explained_variance, alpha=0.7, color='skyblue')\n    cumulative_variance = np.cumsum(explained_variance)\n    ax1.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'ro-', color='red')\n    ax1.set_xlabel('Componente Principal')\n    ax1.set_ylabel('Vari√¢ncia Explicada')\n    ax1.set_title('Vari√¢ncia Explicada por Componente')\n    ax1.grid(True, alpha=0.3)\n    ax1.legend(['Acumulada', 'Individual'])\n    \n    # PC1 vs PC2\n    normal_mask = df['label'] == 0\n    attack_mask = df['label'] == 1\n    \n    ax2.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n               alpha=0.6, c='skyblue', label='Normal', s=30)\n    ax2.scatter(X_pca[attack_mask, 0], X_pca[attack_mask, 1], \n               alpha=0.6, c='salmon', label='Ataque', s=30)\n    ax2.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n    ax2.set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n    ax2.set_title('Proje√ß√£o PCA (PC1 vs PC2)')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Loading plot\n    loadings = pca.components_[:2].T * np.sqrt(pca.explained_variance_[:2])\n    ax3.scatter(loadings[:, 0], loadings[:, 1], alpha=0.7, s=100, color='purple')\n    for i, feature in enumerate(numeric_cols):\n        ax3.annotate(feature, (loadings[i, 0], loadings[i, 1]), \n                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n    ax3.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n    ax3.set_ylabel(f'PC2 ({explained_variance[1]*100:.1f}%)')\n    ax3.set_title('Loading Plot (Contribui√ß√£o das Features)')\n    ax3.grid(True, alpha=0.3)\n    ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n    ax3.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n    \n    # PC1 vs PC3\n    ax4.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 2], \n               alpha=0.6, c='skyblue', label='Normal', s=30)\n    ax4.scatter(X_pca[attack_mask, 0], X_pca[attack_mask, 2], \n               alpha=0.6, c='salmon', label='Ataque', s=30)\n    ax4.set_xlabel(f'PC1 ({explained_variance[0]*100:.1f}%)')\n    ax4.set_ylabel(f'PC3 ({explained_variance[2]*100:.1f}%)')\n    ax4.set_title('Proje√ß√£o PCA (PC1 vs PC3)')\n    ax4.legend()\n    ax4.grid(True, alpha=0.3)\n    \n    plt.suptitle('An√°lise de Componentes Principais (PCA)', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('../figures/04_pca_analysis.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"‚úÖ An√°lise PCA salva em figures/04_pca_analysis.png\")\n\n# 5. Balanceamento das classes\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Contagem por classe\nlabel_counts = df['label'].value_counts().sort_index()\nlabel_props = df['label'].value_counts(normalize=True).sort_index()\n\nbalance_data = pd.DataFrame({\n    'Classe': ['Normal (0)', 'Ataque (1)'],\n    'Quantidade': label_counts.values,\n    'Propor√ß√£o (%)': (label_props.values * 100).round(2)\n})\n\n# Gr√°fico de barras\nbalance_data.plot(x='Classe', y='Quantidade', kind='bar', ax=ax1, color=['skyblue', 'salmon'], legend=False)\nax1.set_title('Distribui√ß√£o Absoluta das Classes', fontweight='bold')\nax1.set_xlabel('Classe')\nax1.set_ylabel('N√∫mero de Fluxos')\nax1.tick_params(axis='x', rotation=0)\n\n# Gr√°fico de pizza\nax2.pie(balance_data['Quantidade'], labels=balance_data['Classe'], autopct='%1.1f%%', \n        colors=['skyblue', 'salmon'], startangle=90)\nax2.set_title('Propor√ß√£o das Classes', fontweight='bold')\n\nplt.suptitle('An√°lise de Balanceamento do Dataset', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.savefig('../figures/05_class_balance.png', dpi=300, bbox_inches='tight')\nplt.close()\nprint(\"‚úÖ An√°lise de balanceamento salva em figures/05_class_balance.png\")\n\n# 6. Pairplot simplificado (principais features)\nif len(numeric_cols) >= 3:\n    # Selecionar 3 features mais importantes para pairplot (economizar espa√ßo)\n    top_features = numeric_cols[:3]  # Primeiras 3 features\n    \n    g = sns.pairplot(data=df[top_features + ['label']], \n                     hue='label', \n                     plot_kws={'alpha': 0.6, 's': 30},\n                     diag_kind='hist',\n                     palette=['skyblue', 'salmon'])\n    \n    g.fig.suptitle('An√°lise Bivariada - Top 3 Features', y=1.02, fontsize=14, fontweight='bold')\n    \n    # Personalizar legendas\n    for ax in g.axes.flat:\n        if ax.legend_:\n            ax.legend(labels=['Normal', 'Ataque'], loc='best')\n    \n    plt.savefig('../figures/06_pairplot_top_features.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"‚úÖ Pairplot das principais features salvo em figures/06_pairplot_top_features.png\")\n\nprint(f\"\\nüé® Total de visualiza√ß√µes salvas: 6\")\nprint(\"üìÅ Todas as visualiza√ß√µes est√£o dispon√≠veis em figures/ para inser√ß√£o no manuscrito\")\n\n# Reset das configura√ß√µes do matplotlib\nplt.rcParams.update(plt.rcParamsDefault)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}