# Arquiteturas Neurais dos Modelos - RepresentaÃ§Ã£o como NeurÃ´nios Reais

## Modelo V1 - Autoencoder BÃ¡sico
```mermaid
graph TB
    subgraph "INPUT LAYER - Dados de Entrada"
        I1[bytes ğŸ”¢]
        I2[pkts ğŸ“¦]
        I3[iat_mean â±ï¸]
    end

    subgraph "ENCODER - CompressÃ£o Progressiva"
        subgraph "Hidden Layer 1 (4Ã—D = 12 neurÃ´nios)"
            H11[N1 ğŸ§ ] 
            H12[N2 ğŸ§ ]
            H13[N3 ğŸ§ ]
            H14[N4 ğŸ§ ]
            H15[...] 
            H16[N12 ğŸ§ ]
        end
        
        subgraph "Hidden Layer 2 (2Ã—D = 6 neurÃ´nios)"
            H21[N1 ğŸ§ ]
            H22[N2 ğŸ§ ]
            H23[N3 ğŸ§ ]
            H24[N4 ğŸ§ ]
            H25[N5 ğŸ§ ]
            H26[N6 ğŸ§ ]
        end
        
        subgraph "BOTTLENECK (D = 3 neurÃ´nios) - RepresentaÃ§Ã£o Compacta"
            B1[Feature 1 ğŸ’]
            B2[Feature 2 ğŸ’]
            B3[Feature 3 ğŸ’]
        end
    end

    subgraph "DECODER - ReconstruÃ§Ã£o Progressiva"
        subgraph "Hidden Layer 3 (2Ã—D = 6 neurÃ´nios)"
            D21[N1 ğŸ§ ]
            D22[N2 ğŸ§ ]
            D23[N3 ğŸ§ ]
            D24[N4 ğŸ§ ]
            D25[N5 ğŸ§ ]
            D26[N6 ğŸ§ ]
        end
        
        subgraph "Hidden Layer 4 (4Ã—D = 12 neurÃ´nios)"
            D11[N1 ğŸ§ ]
            D12[N2 ğŸ§ ]
            D13[N3 ğŸ§ ]
            D14[N4 ğŸ§ ]
            D15[...]
            D16[N12 ğŸ§ ]
        end
    end

    subgraph "OUTPUT LAYER - ReconstruÃ§Ã£o"
        O1[bytes' ğŸ”¢]
        O2[pkts' ğŸ“¦]
        O3[iat_mean' â±ï¸]
    end

    subgraph "ANOMALY DETECTION - DetecÃ§Ã£o"
        AD1[Erro MSE ğŸ“Š]
        AD2[Threshold ğŸ¯]
        AD3[Normal/Anomalia âš ï¸]
    end

    %% ConexÃµes Input -> Encoder
    I1 --> H11
    I1 --> H12
    I1 --> H13
    I2 --> H11
    I2 --> H12
    I2 --> H13
    I3 --> H11
    I3 --> H12
    I3 --> H13

    %% ConexÃµes Encoder
    H11 --> H21
    H12 --> H21
    H13 --> H22
    H14 --> H22
    H21 --> B1
    H22 --> B2
    H23 --> B3

    %% ConexÃµes Decoder
    B1 --> D21
    B2 --> D22
    B3 --> D23
    D21 --> D11
    D22 --> D12
    D23 --> D13

    %% ConexÃµes Output
    D11 --> O1
    D12 --> O2
    D13 --> O3

    %% DetecÃ§Ã£o de Anomalia
    O1 --> AD1
    O2 --> AD1
    O3 --> AD1
    I1 -.-> AD1
    I2 -.-> AD1
    I3 -.-> AD1
    AD1 --> AD2
    AD2 --> AD3

    style I1 fill:#e3f2fd
    style I2 fill:#e3f2fd
    style I3 fill:#e3f2fd
    style B1 fill:#ffcdd2
    style B2 fill:#ffcdd2
    style B3 fill:#ffcdd2
    style AD3 fill:#c8e6c9
```

## Modelo V2/V3 - Pipeline ConfigurÃ¡vel
```mermaid
graph TB
    subgraph "DATA PREPROCESSING - PrÃ©-processamento"
        PP1[ImputaÃ§Ã£o ğŸ”§]
        PP2[TransformaÃ§Ãµes ğŸ”„]
        PP3[NormalizaÃ§Ã£o ğŸ“]
    end

    subgraph "CONFIGURABLE INPUT - Entrada ConfigurÃ¡vel"
        CI1[Feature 1 ğŸ“Š]
        CI2[Feature 2 ğŸ“Š]
        CI3[Feature N ğŸ“Š]
    end

    subgraph "ENCODER CONFIGURÃVEL"
        subgraph "Layer 1 (Multiplicador Ã— D)"
            E11[N1 ğŸ§ ]
            E12[N2 ğŸ§ ]
            E13[Dropout ğŸ²]
            E14[...NM ğŸ§ ]
        end
        
        subgraph "Layer 2 (Multiplicador Ã— D)"
            E21[N1 ğŸ§ ]
            E22[N2 ğŸ§ ]
            E23[Dropout ğŸ²]
            E24[...NK ğŸ§ ]
        end
        
        subgraph "BOTTLENECK ADAPTATIVO"
            EB1[Feature 1 ğŸ’]
            EB2[Feature 2 ğŸ’]
            EB3[Feature K ğŸ’]
        end
    end

    subgraph "DECODER SIMÃ‰TRICO"
        subgraph "Layer 3 (Espelho Layer 2)"
            D21[N1 ğŸ§ ]
            D22[N2 ğŸ§ ]
            D23[Dropout ğŸ²]
            D24[...NK ğŸ§ ]
        end
        
        subgraph "Layer 4 (Espelho Layer 1)"
            D11[N1 ğŸ§ ]
            D12[N2 ğŸ§ ]
            D13[Dropout ğŸ²]
            D14[...NM ğŸ§ ]
        end
    end

    subgraph "RECONSTRUCTION OUTPUT"
        RO1[Feature 1' ğŸ“Š]
        RO2[Feature 2' ğŸ“Š]
        RO3[Feature N' ğŸ“Š]
    end

    subgraph "ADVANCED ANOMALY DETECTION"
        AAD1[MÃºltiplos Thresholds ğŸ¯]
        AAD2[P95, P97, Î¼+2Ïƒ ğŸ“ˆ]
        AAD3[Threshold Conservador ğŸ›¡ï¸]
        AAD4[Normal/Anomalia âš ï¸]
    end

    %% Fluxo de dados
    PP1 --> PP2
    PP2 --> PP3
    PP3 --> CI1
    PP3 --> CI2
    PP3 --> CI3

    CI1 --> E11
    CI2 --> E12
    CI3 --> E14

    E11 --> E21
    E12 --> E22
    E14 --> E24

    E21 --> EB1
    E22 --> EB2
    E24 --> EB3

    EB1 --> D21
    EB2 --> D22
    EB3 --> D24

    D21 --> D11
    D22 --> D12
    D24 --> D14

    D11 --> RO1
    D12 --> RO2
    D14 --> RO3

    RO1 --> AAD1
    RO2 --> AAD1
    RO3 --> AAD1
    AAD1 --> AAD2
    AAD2 --> AAD3
    AAD3 --> AAD4

    style PP1 fill:#f3e5f5
    style PP2 fill:#f3e5f5
    style PP3 fill:#f3e5f5
    style EB1 fill:#ffcdd2
    style EB2 fill:#ffcdd2
    style EB3 fill:#ffcdd2
    style AAD4 fill:#c8e6c9
```

## Modelo V4 - LSTM-Attention Temporal
```mermaid
graph TB
    subgraph "SEQUENCE INPUT - Entrada Temporal"
        subgraph "Timestep 1"
            T11[bytes ğŸ”¢]
            T12[pkts ğŸ“¦]
            T13[iat_mean â±ï¸]
            T14[duration ğŸ•]
            T15[iat_std ğŸ“]
        end
        subgraph "Timestep 2"
            T21[bytes ğŸ”¢]
            T22[pkts ğŸ“¦]
            T23[iat_mean â±ï¸]
            T24[duration ğŸ•]
            T25[iat_std ğŸ“]
        end
        subgraph "..."
            TX1[... â‹¯]
        end
        subgraph "Timestep 10"
            T101[bytes ğŸ”¢]
            T102[pkts ğŸ“¦]
            T103[iat_mean â±ï¸]
            T104[duration ğŸ•]
            T105[iat_std ğŸ“]
        end
    end

    subgraph "LSTM ENCODER - Processamento Temporal"
        subgraph "LSTM Layer 1 (64 unidades)"
            L11[LSTM Cell 1 ğŸ§ ğŸ“š]
            L12[LSTM Cell 2 ğŸ§ ğŸ“š]
            L13[Hidden State hâ‚ ğŸ§­]
            L14[Cell State câ‚ ğŸ“–]
        end
        
        subgraph "LSTM Layer 2 (32 unidades)"
            L21[LSTM Cell 1 ğŸ§ ğŸ“š]
            L22[LSTM Cell 2 ğŸ§ ğŸ“š]
            L23[Hidden State hâ‚‚ ğŸ§­]
            L24[Cell State câ‚‚ ğŸ“–]
        end
        
        subgraph "ATTENTION MECHANISM - Mecanismo de AtenÃ§Ã£o"
            A1[Query ğŸ”]
            A2[Key ğŸ—ï¸]
            A3[Value ğŸ’]
            A4[Attention Weights ğŸ¯]
            A5[Context Vector ğŸŒŸ]
        end
    end

    subgraph "BOTTLENECK TEMPORAL"
        TB1[Temporal Feature 1 â°]
        TB2[Temporal Feature 2 â°]
        TB3[Temporal Feature 8 â°]
    end

    subgraph "LSTM DECODER - ReconstruÃ§Ã£o Temporal"
        subgraph "Repeat Vector"
            RV1[Broadcast ğŸ“¡]
        end
        
        subgraph "LSTM Layer 3 (32 unidades)"
            LD1[LSTM Cell 1 ğŸ§ ğŸ“š]
            LD2[LSTM Cell 2 ğŸ§ ğŸ“š]
            LD3[Hidden State hâ‚ƒ ğŸ§­]
        end
        
        subgraph "LSTM Layer 4 (64 unidades)"
            LD4[LSTM Cell 1 ğŸ§ ğŸ“š]
            LD5[LSTM Cell 2 ğŸ§ ğŸ“š]
            LD6[Hidden State hâ‚„ ğŸ§­]
        end
    end

    subgraph "SEQUENCE OUTPUT - SaÃ­da Temporal"
        subgraph "Reconstructed Timestep 1"
            RT11[bytes' ğŸ”¢]
            RT12[pkts' ğŸ“¦]
            RT13[iat_mean' â±ï¸]
            RT14[duration' ğŸ•]
            RT15[iat_std' ğŸ“]
        end
        subgraph "..."
            RTX1[... â‹¯]
        end
        subgraph "Reconstructed Timestep 10"
            RT101[bytes' ğŸ”¢]
            RT102[pkts' ğŸ“¦]
            RT103[iat_mean' â±ï¸]
            RT104[duration' ğŸ•]
            RT105[iat_std' ğŸ“]
        end
    end

    subgraph "TEMPORAL ANOMALY DETECTION"
        TAD1[Sequence MSE ğŸ“Š]
        TAD2[Temporal Patterns ğŸŒŠ]
        TAD3[Attention Analysis ğŸ”]
        TAD4[Anomaly Score âš ï¸]
    end

    %% ConexÃµes Temporais
    T11 --> L11
    T21 --> L11
    T101 --> L11
    
    L11 --> L21
    L12 --> L21
    L13 --> L21
    
    L21 --> A1
    L22 --> A2
    L23 --> A3
    A1 --> A4
    A2 --> A4
    A3 --> A4
    A4 --> A5
    
    A5 --> TB1
    A5 --> TB2
    A5 --> TB3
    
    TB1 --> RV1
    TB2 --> RV1
    TB3 --> RV1
    
    RV1 --> LD1
    LD1 --> LD4
    LD4 --> RT11
    LD4 --> RT101
    
    RT11 --> TAD1
    RT101 --> TAD1
    TAD1 --> TAD2
    TAD2 --> TAD3
    TAD3 --> TAD4

    style T11 fill:#e8eaf6
    style T21 fill:#e8eaf6
    style T101 fill:#e8eaf6
    style A5 fill:#fff3e0
    style TB1 fill:#ffcdd2
    style TB2 fill:#ffcdd2
    style TB3 fill:#ffcdd2
    style TAD4 fill:#c8e6c9
```

## ComparaÃ§Ã£o de Processamento Neural

### Modelo V1-V3: Processamento Pontual
- **Entrada**: 1 fluxo â†’ 3-5 features
- **Processamento**: NeurÃ´nios densos com ativaÃ§Ã£o ReLU
- **MemÃ³ria**: Sem memÃ³ria temporal
- **DetecÃ§Ã£o**: Erro de reconstruÃ§Ã£o instantÃ¢neo

### Modelo V4: Processamento Temporal
- **Entrada**: SequÃªncia de 10 fluxos â†’ 50 features temporais
- **Processamento**: LSTM cells com gates (forget, input, output)
- **MemÃ³ria**: Hidden states + Cell states para contexto temporal
- **AtenÃ§Ã£o**: Foco dinÃ¢mico em timesteps relevantes
- **DetecÃ§Ã£o**: PadrÃµes temporais anÃ´malos

## Analogia com NeurÃ´nios Reais

### NeurÃ´nios Densos (V1-V3)
- ğŸ§  **Como neurÃ´nios simples**: Recebem sinais, processam com funÃ§Ã£o de ativaÃ§Ã£o
- âš¡ **Sinapses**: Pesos conectam todas as entradas a todas as saÃ­das
- ğŸ”„ **Processamento**: InstantÃ¢neo, sem memÃ³ria

### LSTM Cells (V4)
- ğŸ§ ğŸ“š **Como neurÃ´nios com memÃ³ria**: MantÃªm informaÃ§Ã£o ao longo do tempo
- ğŸšª **Gates**: Controlam fluxo de informaÃ§Ã£o (como canais iÃ´nicos)
- ğŸ“– **Cell State**: MemÃ³ria de longo prazo (como potencial de repouso)
- ğŸ§­ **Hidden State**: MemÃ³ria de curto prazo (como potencial de aÃ§Ã£o)

### Attention Mechanism (V4)
- ğŸ” **Como atenÃ§Ã£o seletiva**: Foca em informaÃ§Ãµes relevantes
- ğŸ¯ **Pesos de atenÃ§Ã£o**: Determina importÃ¢ncia de cada timestep
- ğŸŒŸ **Context vector**: Resumo ponderado das informaÃ§Ãµes importantes